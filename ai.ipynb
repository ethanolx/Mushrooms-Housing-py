{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd006e46f44bb844bdd55de1af887732b14e875f5b0d2374a3f4b9484cbcb737e08",
   "display_name": "Python 3.8.5  ('.venv': pipenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "06e46f44bb844bdd55de1af887732b14e875f5b0d2374a3f4b9484cbcb737e08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "source": [
    "# AIML CA1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import General Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Dependencies\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Graphing Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Dependencies\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Preservation Dependency\n",
    "import pickle\n",
    "\n",
    "# Miscellaneous Dependencies\n",
    "from typing import Callable, Dict, Union    # static typing\n",
    "from warnings import filterwarnings         # warnings\n",
    "\n",
    "# Utility Functions\n",
    "from utils.extraction import extract_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide Warnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "source": [
    "## Part A > Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How is your prediction task defined? And what is the meaning of the\n",
    "output variable?\n",
    "\n",
    "```\n",
    "    The task is to predict if a mushroom of the agaricus lepiota family is edible or poisonous,\n",
    "    given its properties (i.e. cap-shape, odor, etc.)\n",
    "\n",
    "    The output variable is class, and its possible values carry the respective meanings:\n",
    "    'edible':       the mushroom is safe for consumption\n",
    "    'poisonous':    do not consume the mushroom\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Data\n",
    "\n",
    "Load data about edibility of gilled mushrooms of the agaricus lepiota family"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_mushrooms() -> pd.DataFrame:\n",
    "    # Extract raw content of ./data/agaricus-lepiota.names file\n",
    "    metadata: str\n",
    "    with open('./data/agaricus-lepiota.names') as f:\n",
    "        metadata = f.read()\n",
    "\n",
    "    # Extract attributes from metadata\n",
    "    attrs = extract_attributes(metadata, r'7\\. Attribute Information:.*\\n((.|\\n)*)8\\. Missing')\n",
    "\n",
    "    # Extract column names to be used for dataframe\n",
    "    cols = attrs.keys()\n",
    "\n",
    "    # Create the dataframe from ./data/agaricus-lepiota.data file,\n",
    "    #   using column names derived from ./data/agaricus-lepiota.names file\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer='./data/agaricus-lepiota.data',\n",
    "        sep=',',\n",
    "        header=0,\n",
    "        names=cols\n",
    "    )\n",
    "\n",
    "    # Expand attribute codes to their full definitions\n",
    "    for col in cols:\n",
    "        df[col].replace(to_replace=attrs[col] ,inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_mushrooms()"
   ]
  },
  {
   "source": [
    "#### Inspect Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How do you represent your data as features?\n",
    "\n",
    "```\n",
    "    I represent the features as columns in a pandas DataFrame\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect top 10 rows of the dataset\n",
    "df.head(n=10)"
   ]
  },
  {
   "source": [
    "#### Summarize Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect overview of the dataset\n",
    "df.info()"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   Did you process the features in any way?\n",
    "\n",
    "```\n",
    "    Yes, the features underwent (feature) selection and (one-hot) encoding\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to keep track of variables to be removed\n",
    "drop_cols = []"
   ]
  },
  {
   "source": [
    "Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isna().sum(axis=0)\n",
    "\n",
    "# Note that stalk-root has missing attributes (denoted as 'missing')\n",
    "# In fact, approx. 31% of the records have missing data for stalk-root\n",
    "stalk_dist = df['stalk-root'].value_counts()\n",
    "(stalk_dist / stalk_dist.sum()).round(2)\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols.append('stalk-root')"
   ]
  },
  {
   "source": [
    "Redundant Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unqiue counts of the individual features\n",
    "print(df.describe().transpose().sort_values(by='unique', ascending=False))\n",
    "\n",
    "# Note that veil-type has only one value,\n",
    "#   hence it is a redundant feature\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols.append('veil-type')"
   ]
  },
  {
   "source": [
    "Inspect the distribution of the target variable (class: edible/poisonous)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import format_label\n",
    "def plot_A(df: pd.DataFrame):\n",
    "    ax = sns.countplot(data=df, x='class', palette='deep')\n",
    "    ax.set_ylim(top=5000)\n",
    "    ax.set_title(label='General Data Distribution')\n",
    "    ax.set_ylabel(ylabel='Number of Records')\n",
    "    ax.set_yticklabels(labels=format_label(\n",
    "        ax.get_yticks() / 1000, lambda s: f'{round(s)}k'))\n",
    "    ax.set_xlabel(xlabel='Type')\n",
    "    total_count = df.shape[0]\n",
    "    for p in ax.patches:\n",
    "        x = p.get_x()\n",
    "        y = p.get_height()\n",
    "        ax.annotate(text=f'{y} ({y/total_count*100:.1f}%)',\n",
    "                    xy=(x + 0.21, y + 70))\n",
    "    return ax\n",
    "# plot_A(df=df)"
   ]
  },
  {
   "source": [
    "Inspect correlation between the independent variables and the target variable (class)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_B(df: pd.DataFrame):\n",
    "    for i in df.drop(labels='class', axis=1).columns.values:\n",
    "        fig, (corr_plot, freq_plot) = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "        ct = pd.crosstab(index=df['class'], columns=df[i])\n",
    "        distr = df.groupby(i).count().iloc[:,0]\n",
    "        proportion = (ct.iloc[1] - ct.iloc[0]) / distr\n",
    "        corr = pd.DataFrame(proportion.reset_index())\n",
    "        sns.barplot(data=corr, x=i, y=0, ax=corr_plot, color='grey')\n",
    "        sns.countplot(data=df.sort_values(by=i), x=i, hue='class', ax=freq_plot, palette='turbo')\n",
    "        fig.suptitle(t=f'{i.upper()}')\n",
    "        corr_plot.set_title(label='Correlation (chi2-based)')\n",
    "        corr_plot.set_ylim((-1.1, 1.1))\n",
    "        corr_plot.set_ylabel(ylabel='Correlation')\n",
    "        corr_plot.set_xticklabels(labels=corr_plot.get_xticklabels(), rotation=30)\n",
    "        freq_plot.set_title(label=f'Frequency Distribution')\n",
    "        freq_plot.set_xticklabels(labels=freq_plot.get_xticklabels(), rotation=30)\n",
    "# plot_B(df)"
   ]
  },
  {
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "There is no need for feature engineering in this dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Feature Selection\n",
    "\n",
    "There are 2 columns to be removed (stalk-root, veil-type)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns\n",
    "df.drop(labels=drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "### Encoding the data\n",
    "\n",
    "The data has only categorical text variables, therefore they<br>have to be converted to numeric form using dummy variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (One-Hot) Encode the dataset (categorical -> binary)\n",
    "df_ohe = pd.get_dummies(data=df, drop_first=True)"
   ]
  },
  {
   "source": [
    "### Inspect correlation after encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correlation between top 10 factors and target variable (class)\n",
    "df_ohe.corr()['class_poisonous'].drop(labels='class_poisonous').sort_values(key=lambda x: np.abs(x), ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi2-based feature selection\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Get top 10 factors that are correlated with the target variable (class)\n",
    "best_features_chi2 = SelectKBest(score_func=chi2, k=10).fit(X=df_ohe.drop(labels='class_poisonous', axis=1), y=df_ohe['class_poisonous'])\n",
    "best_features_mask = best_features_chi2.get_support()\n",
    "best_features = df_ohe.drop(labels='class_poisonous', axis=1).columns.values[best_features_mask]\n",
    "best_features_scores = best_features_chi2.scores_[best_features_mask]\n",
    "good_predictors = pd.Series(data=best_features_scores, index=best_features)\n",
    "\n",
    "good_predictors.sort_values(ascending=False)"
   ]
  },
  {
   "source": [
    "### Data Partitioning\n",
    "\n",
    "Split the data randomly into a train set and a test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X = df_ohe.drop(labels='class_poisonous', axis=1)\n",
    "y = df_ohe['class_poisonous']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "source": [
    "### Algorithm Selection & Hyper-Parameter Tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate classification algorithms\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "source": [
    "#### Determine best candidate algorithm using GridSearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_clf():\n",
    "    cand_pipe_1 = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', DummyEstimator())\n",
    "    ])\n",
    "\n",
    "    cand_params_1 = [\n",
    "        {\n",
    "            'clf': [KNeighborsClassifier()],\n",
    "            'clf__n_neighbors': np.arange(3, 14, 2)\n",
    "        },\n",
    "        {\n",
    "            'clf': [LogisticRegression()],\n",
    "            'clf__solver': ['liblinear', 'newton-cg'],\n",
    "            'clf__C': np.logspace(-3, 3, 3),\n",
    "            'clf__multi_class': ['ovr']\n",
    "        },\n",
    "        {\n",
    "            'clf': [CategoricalNB()],\n",
    "            'clf__alpha': np.logspace(-3, 3, 6)\n",
    "        },\n",
    "        {\n",
    "            'clf': [SVC()],\n",
    "            'clf__kernel': ['rbf', 'poly'],\n",
    "            'clf__C': np.logspace(-3, 4, 3)\n",
    "        },\n",
    "        {\n",
    "            'clf': [DecisionTreeClassifier()],\n",
    "            'clf__max_depth': [10, 20, 30],\n",
    "            'clf__min_samples_leaf': [10, 30]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    best_clf_algo = GridSearchCV(estimator=cand_pipe_1, param_grid=cand_params_1, cv=3)\n",
    "    best_clf_algo.fit(X=X, y=y)\n",
    "    return best_clf_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=grid_search_clf(), file=open('./models/best_clf_algo.p', 'wb'))\n",
    "\n",
    "# Load result\n",
    "best_clf_algo_loaded = pickle.load(file=open('./models/best_clf_algo.p', 'rb'))\n",
    "\n",
    "# Inspect result\n",
    "print(best_clf_algo_loaded.best_estimator_)\n",
    "gs_clf = pd.DataFrame(best_clf_algo_loaded.cv_results_)\n",
    "gs_clf.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "#### Determine best hyperparameters for selected algorithm using GridSearch\n",
    "\n",
    "Selected algorithm: logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_clf_params():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('scaler', DummyScaler()),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    params = {\n",
    "        'scaler': ['passthrough', StandardScaler()],\n",
    "        'clf__solver': ['liblinear', 'saga'],\n",
    "        'clf__tol': np.logspace(-5, 2, 3),\n",
    "        'clf__C': np.logspace(-4, 4, 5),\n",
    "        'clf__multi_class': ['ovr']\n",
    "    }\n",
    "\n",
    "    best_clf_params = GridSearchCV(estimator=pipe, param_grid=params, cv=5, n_jobs=-1)\n",
    "    best_clf_params.fit(X=X, y=y)\n",
    "    return best_clf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=grid_search_clf_params(), file=open('./models/best_clf_params.p', 'wb'))\n",
    "\n",
    "# Load result\n",
    "best_clf_params = pickle.load(file=open('./models/best_clf_params.p', 'rb'))\n",
    "\n",
    "# Inspect result\n",
    "print(best_clf_params.best_params_['clf'])\n",
    "gs_clf_params = pd.DataFrame(best_clf_params.cv_results_)\n",
    "gs_clf_params.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "### Check for Overfitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_overfit_clf(model: LogisticRegression, cv: int):\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        data=np.vstack((\n",
    "            cross_val_score(estimator=model, X=X_train, y=y_train, cv=cv),\n",
    "            cross_val_score(estimator=model, X=X_train, y=y_train, cv=cv)\n",
    "        )),\n",
    "        columns=[f'Test {i + 1}' for i in range(cv)],\n",
    "        index=['Train Set', 'Test Set']\n",
    "    )\n",
    "\n",
    "test_overfit_clf(model=LogisticRegression(C=100.0, multi_class='ovr', solver='liblinear', tol=1e-05), cv=6)"
   ]
  },
  {
   "source": [
    "### Building Pipeline\n",
    "<br>\n",
    "Build a machine learning pipeline, using\n",
    "\n",
    "*   a custom feature-selection transformer,\n",
    "*   a one-hot encoder,\n",
    "*   the most consistent algorithm,\n",
    "*   the best performing hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_cols_1(df: pd.DataFrame):\n",
    "    return df.drop(labels=drop_cols, axis=1, errors='ignore')\n",
    "\n",
    "class FeatureSelector1(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return drop_redundant_cols_1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Specify all possible column values for dataset\n",
    "def get_column_values(df: pd.DataFrame):\n",
    "    categories = []\n",
    "    for i in df.drop(labels=['class', *drop_cols], axis=1, errors='ignore').columns.values:\n",
    "        categories.append(pd.unique(df[i]))\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[\n",
    "    ('feature_selector', FeatureSelector1()),\n",
    "    ('encoder', OneHotEncoder(categories=get_column_values(df))),\n",
    "    ('classifier', LogisticRegression(C=100.0, multi_class='ovr', solver='liblinear', tol=1e-05))\n",
    "])"
   ]
  },
  {
   "source": [
    "### Redefine Data Partition\n",
    "\n",
    "With the relevant transformers in place, data pre-processing<br>has been integrated into the machine learning pipeline\n",
    "\n",
    "Therefore, the data should be retrieved from the original source and re-partitioned"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_mushrooms()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(labels='class', axis=1), df['class'], random_state=2)"
   ]
  },
  {
   "source": [
    "### Model Training\n",
    "\n",
    "Fit the data to the pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# pickle.dump(obj=model, file=open('./models/final_classifier.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "final_classifier = pickle.load(file=open('./models/final_classifier.p', 'rb'))"
   ]
  },
  {
   "source": [
    "### Model Scoring\n",
    "\n",
    "Use the model to generate predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_classifier.predict(X=X_test)\n",
    "y_pred"
   ]
  },
  {
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluate the performance of the final model based on standard classification metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model evaluation dependencies\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "source": [
    "#### Evaluate against train set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = final_classifier.predict(X=X_train)\n",
    "\n",
    "# Classification summary\n",
    "print(classification_report(y_true=y_train, y_pred=y_train_pred, target_names=['edible', 'poisonous']))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\n', pd.DataFrame(data=confusion_matrix(y_true=y_train, y_pred=y_train_pred, labels=['edible', 'poisonous']), index=['Actual Edible', 'Actual Poisonous'], columns=['Predicted Edible', 'Predicted Poisonous']), '\\n\\n', sep='')\n",
    "\n",
    "# Print target distribution in y_test\n",
    "print(y_train.groupby(y_train).count())"
   ]
  },
  {
   "source": [
    "#### Evaluate against test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification summary\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred, target_names=['edible', 'poisonous']))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\n', pd.DataFrame(data=confusion_matrix(y_true=y_test, y_pred=y_pred, labels=['edible', 'poisonous']), index=['Actual Edible', 'Actual Poisonous'], columns=['Predicted Edible', 'Predicted Poisonous']), '\\n\\n', sep='')\n",
    "\n",
    "# Print target distribution in y_test\n",
    "print(y_test.groupby(y_test).count())"
   ]
  },
  {
   "source": [
    "## Part B > Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Data\n",
    "\n",
    "Load data about King County house sales"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from a csv file\n",
    "df2 = pd.read_csv('./data/kc_house_data.csv')"
   ]
  },
  {
   "source": [
    "#### Inspect Data\n",
    "\n",
    "Preview a sample of the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the top 10 rows of the dataset\n",
    "df2.head(n=10)"
   ]
  },
  {
   "source": [
    "#### Summarize Data\n",
    "\n",
    "Get a sense of the features involved"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect overview of the dataset\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect statistics of the dataset\n",
    "df2.describe().transpose().round(2)"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Exploratory Data Analysis (EDA)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to keep track of variables to be removed\n",
    "drop_cols_2 = []\n",
    "\n",
    "# List to keep track of positively skewed variables\n",
    "positively_skewed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df2.isna().sum(axis=0)\n",
    "\n",
    "# There doesn't seem to be any missing values"
   ]
  },
  {
   "source": [
    "Visualize correlation amongst the original features using a heatmap"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2A():\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(data=df2.corr(), cmap='RdBu', vmin=-1, vmax=1, ax=ax)\n",
    "# plot_2A()"
   ]
  },
  {
   "source": [
    "Inspect distribution of the individual variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2B():\n",
    "    for i in df2.columns.values:\n",
    "        if df2[i].dtype.kind in 'biufc':\n",
    "            fig, (hst, bxp) = plt.subplots(ncols=2)\n",
    "            sns.histplot(data=df2, x=i, ax=hst)\n",
    "            sns.boxplot(data=df2, y=i, ax=bxp)\n",
    "# plot_2B()\n",
    "\n",
    "# Many of the features seem to be positively skewed\n",
    "\n",
    "# Course of action - logarithmic transformation\n",
    "positively_skewed.extend(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'long', 'sqft_living15', 'sqft_lot15'])"
   ]
  },
  {
   "source": [
    "Inspect correlation between the features and the target variable (price)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_E(df: pd.DataFrame):\n",
    "    for i in positively_skewed:\n",
    "        perc = df[df[i] == 0].shape[0] / df.shape[0] * 100\n",
    "        fig, (bef, aft) = plt.subplots(ncols=2)\n",
    "        bef.set_title(label='Before Logarithmic Transformation')\n",
    "        aft.set_title(label='After Logarithmic Transformation')\n",
    "        sns.histplot(data=df, x=i, ax=bef)\n",
    "        sns.histplot(data=np.log1p(df[i]), ax=aft)\n",
    "# plot_E(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr()['price'].sort_values(key=lambda x: np.abs(x), ascending=False)"
   ]
  },
  {
   "source": [
    "Inspect absolute correlation between features and target variable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D():\n",
    "    fe = FeatureEngineering()\n",
    "    fs = FeatureSelection()\n",
    "    sns.heatmap(fe.fit_transform(pd.read_csv('./data/kc_house_data.csv')).corr().abs(), vmin=0, vmax=1, cmap='Blues')\n",
    "# plot_2D()"
   ]
  },
  {
   "source": [
    "Inspect id feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check id data type\n",
    "print('ID data type:\\t\\t', df2['id'].dtype)\n",
    "\n",
    "# Compare the number of ids to the total number of records \n",
    "print('Number of unique IDs:\\t', pd.unique(df2['id']).size)\n",
    "print('Total no. of records:\\t', df2.shape[0], '\\n')\n",
    "\n",
    "# Check correlation between id and the rest of the variables\n",
    "print(df2.corr()['id'].sort_values(key=lambda x: np.abs(x), ascending=False))\n",
    "\n",
    "\n",
    "# id seems redundant\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols_2.append('id')"
   ]
  },
  {
   "source": [
    "Inspect zipcode feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check zipcode data type\n",
    "print('zipcode data type:\\t\\t', df2['zipcode'].dtype)\n",
    "\n",
    "# Compare the number of zipcodes to the total number of records \n",
    "print('Number of unique zipcodes:\\t', pd.unique(df2['zipcode']).size)\n",
    "print('Total no. of records:\\t', df2.shape[0], '\\n')\n",
    "\n",
    "# Check correlation between zipcode and the rest of the variables\n",
    "print(df2.corr()['zipcode'].sort_values(key=lambda x: np.abs(x), ascending=False))\n",
    "\n",
    "\n",
    "# zipcode does not seem redundant\n",
    "\n",
    "# Course of action - no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_F():\n",
    "    for i in positively_skewed:\n",
    "        fig, (bef, aft, aft_ag) = plt.subplots(ncols=3)\n",
    "        sns.scatterplot(data=df2, x=i, y='price', ax=bef)\n",
    "        sns.scatterplot(x=np.sqrt(df2[i]), y=np.sqrt(df2['price']), ax=aft)\n",
    "        sns.scatterplot(x=(df2[i]), y=np.sqrt(df2['price']), ax=aft_ag)\n",
    "# plot_F()"
   ]
  },
  {
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "There seems to be useful extractable data in the `date` feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month and day from the date feature\n",
    "df2_date = pd.to_datetime(df2['date'], yearfirst=True)\n",
    "df2['year'] = pd.DatetimeIndex(data=df2_date).year\n",
    "df2['month'] = pd.DatetimeIndex(data=df2_date).month\n",
    "df2['day'] = pd.DatetimeIndex(data=df2_date).day\n",
    "\n",
    "# Date variable seems redundant now\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols_2.append('date')"
   ]
  },
  {
   "source": [
    "#### Feature Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review columns to be dropped\n",
    "drop_cols_2"
   ]
  },
  {
   "source": [
    "There are 2 columns to be removed (`id`, `date`)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns\n",
    "df2.drop(labels=drop_cols_2, axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "### Data Partitioning\n",
    "\n",
    "Split the data randomly into a train set and a test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df2.drop(labels='price', axis=1)\n",
    "y2 = df2['price']\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=3)"
   ]
  },
  {
   "source": [
    "### Algorithm Selection & Hyper-Parameter Tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Determine best regression algorithm using GridSearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate regression algorithms\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_reg():\n",
    "    cand_pipe_1 = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', DummyEstimator())\n",
    "    ])\n",
    "\n",
    "    cand_params_1 = [\n",
    "        {\n",
    "            'reg': [LinearRegression()],\n",
    "            'reg__normalize': [True, False],\n",
    "            'reg__fit_intercept': [True, False]\n",
    "        },\n",
    "        {\n",
    "            'reg': [Lasso(), Ridge()],\n",
    "            'reg__alpha': np.logspace(-5, 3, 6)\n",
    "        },\n",
    "        {\n",
    "            'reg': [DecisionTreeRegressor(), GradientBoostingRegressor()],\n",
    "            'reg__max_depth': np.arange(5, 11)\n",
    "        },\n",
    "        {\n",
    "            'reg': [KNeighborsRegressor()],\n",
    "            'reg__n_neighbors': np.arange(5, 11)\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    best_reg_algo = GridSearchCV(estimator=cand_pipe_1, param_grid=cand_params_1, cv=3, n_jobs=-1)\n",
    "    best_reg_algo.fit(X=X2, y=y2)\n",
    "    return best_reg_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=grid_search_reg(), file=open('./models/best_reg_algo.p', 'wb'))\n",
    "\n",
    "# Inspect result\n",
    "best_reg_algo_loaded = pickle.load(file=open('./models/best_reg_algo.p', 'rb'))\n",
    "\n",
    "print(best_reg_algo_loaded.best_estimator_)\n",
    "gs_reg = pd.DataFrame(best_reg_algo_loaded.cv_results_)\n",
    "gs_reg.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "Determine best hyperparameters for selected algorithm using GridSearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_reg_params():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', GradientBoostingRegressor())\n",
    "    ])\n",
    "\n",
    "    params = {\n",
    "        'reg__max_depth': np.arange(2, 5),\n",
    "        'reg__min_samples_split': np.arange(9, 100, 30),\n",
    "        'reg__min_samples_leaf': np.arange(9, 100, 30)\n",
    "    }\n",
    "\n",
    "    best_reg_params = GridSearchCV(estimator=pipe, param_grid=params, cv=5, n_jobs=-1)\n",
    "    best_reg_params.fit(X=X2, y=y2)\n",
    "    return best_reg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=grid_search_reg_params(), file=open('./models/best_reg_params.p', 'wb'))\n",
    "\n",
    "# Inspect result\n",
    "best_reg_params = pickle.load(file=open('./models/best_reg_params.p', 'rb'))\n",
    "\n",
    "print(best_reg_params.best_params_)\n",
    "gs_reg_params = pd.DataFrame(best_reg_params.cv_results_)\n",
    "gs_reg_params.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "Further tuning - Normalizing X Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        all_cols = X.columns.values\n",
    "        for i in positively_skewed:\n",
    "            if i != 'long':\n",
    "                X_copy[i] = np.log1p(X[i])\n",
    "        return X_copy\n",
    "\n",
    "class SqrtTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        all_cols = X.columns.values\n",
    "        for i in positively_skewed:\n",
    "            if i != 'long':\n",
    "                X_copy[i] = np.sqrt(X[i])\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_transformer():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('trans', DummyTransformer()),\n",
    "        ('scaler', DummyScaler()),\n",
    "        ('reg', GradientBoostingRegressor(max_depth=4, min_samples_leaf=9, min_samples_split=9))\n",
    "    ])\n",
    "\n",
    "    params = {\n",
    "        'trans': ['passthrough', LogTransformer(), SqrtTransformer()],\n",
    "        'scaler': ['passthrough', StandardScaler(), RobustScaler()]\n",
    "    }\n",
    "\n",
    "    best_trans_params = GridSearchCV(estimator=pipe, param_grid=params, cv=5, n_jobs=-1)\n",
    "    best_trans_params.fit(X=X2, y=y2)\n",
    "    return best_trans_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=best_transformer(), file=open('./models/best_reg_trans.p', 'wb'))\n",
    "\n",
    "# Inspect result\n",
    "best_reg_trans = pickle.load(file=open('./models/best_reg_trans.p', 'rb'))\n",
    "\n",
    "print(best_reg_trans.best_params_)\n",
    "gs_reg_trans = pd.DataFrame(best_reg_trans.cv_results_)\n",
    "gs_reg_trans.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "Further tuning - Normalizing y Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def further_tune_reg(cv: int = 4):\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('trans', SqrtTransformer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', GradientBoostingRegressor(max_depth=4, min_samples_leaf=9, min_samples_split=9))\n",
    "    ])\n",
    "\n",
    "    sqrt_y = TransformedTargetRegressor(regressor=pipe, func=np.sqrt, inverse_func=np.square)\n",
    "\n",
    "    log_y = TransformedTargetRegressor(regressor=pipe, func=np.log1p, inverse_func=np.expm1)\n",
    "\n",
    "    scores = []\n",
    "    for mo in (pipe, sqrt_y, log_y):\n",
    "        scores.append(cross_val_score(estimator=mo, X=X2, y=y2, cv=cv))\n",
    "    result = pd.DataFrame(data=scores, columns=[f'Test {i + 1}' for i in range(cv)], index=['no y transformation', 'sqrt y transformation', 'log y transformation'])\n",
    "    result['Mean Score'] = result.mean(axis=1)\n",
    "    result['Std Score'] = result.std(axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=further_tune_reg(), file=open('./models/best_reg_y_trans.p', 'wb'))\n",
    "\n",
    "# Inspect result\n",
    "best_reg_y_trans = pickle.load(file=open('./models/best_reg_y_trans.p', 'rb'))\n",
    "\n",
    "print(best_reg_y_trans.sort_values(by='Mean Score', ascending=False))"
   ]
  },
  {
   "source": [
    "### Combining everything"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Building the pipeline\n",
    "<br>\n",
    "Build the machine learning pipeline, using\n",
    "\n",
    "*   a custom feature-engineering transformer,\n",
    "*   a custom feature-selection transformer,\n",
    "*   a custom logarithmic transformer,\n",
    "*   a standard scaler,\n",
    "*   the most consistent algorithm (gradient boosting regressor),\n",
    "*   the best performing hyperparameters\n",
    "\n",
    "To further improve performance and reduce overfitting,<br>\n",
    "the target variable will be transformed too (sqrt)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_parts(df: pd.DataFrame, col: str, **kwargs):\n",
    "    df_datetime = pd.DatetimeIndex(df[col], **kwargs)\n",
    "    return df_datetime.year, df_datetime.month, df_datetime.day\n",
    "\n",
    "class FeatureEngineer2(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_year, X_month, X_day = extract_date_parts(df=X, col='date', yearfirst=True)\n",
    "        X_copy['year'] = X_year\n",
    "        X_copy['month'] = X_month\n",
    "        X_copy['day'] = X_day\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_features(df: pd.DataFrame):\n",
    "    return df.drop(labels=['id', 'zipcode', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "def exclude_target_variable(df: pd.DataFrame):\n",
    "    return df.drop(labels='price', axis=1)\n",
    "\n",
    "class FeatureSelector2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.has_target_variable: bool = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if 'price' in X.columns.values:\n",
    "            self.has_target_variable = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = drop_redundant_features(X)\n",
    "        if self.has_target_variable:\n",
    "            X_copy = exclude_target_variable(X_copy)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipe2 = Pipeline(steps=[\n",
    "    ('feature_engineer', FeatureEngineer2()),\n",
    "    ('feature_selector', FeatureSelector2()),\n",
    "    ('sqrt_transformer', SqrtTransformer()),\n",
    "    ('standard_scaler', StandardScaler()),\n",
    "    ('regressor', GradientBoostingRegressor(max_depth=4, min_samples_leaf=9, min_samples_split=9))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap pipeline in a target transformer\n",
    "model2 = TransformedTargetRegressor(regressor=pipe2, func=np.sqrt, inverse_func=np.square, check_inverse=False)"
   ]
  },
  {
   "source": [
    "### Redefine Data Partition\n",
    "\n",
    "With the relevant transformers in place, data pre-processing<br>has been integrated into the machine learning pipeline\n",
    "\n",
    "Therefore, the data should be retrieved from the original source and re-partitioned"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./data/kc_house_data.csv')\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(df2.drop(labels='price', axis=1), df2['price'], random_state=4)"
   ]
  },
  {
   "source": [
    "### Model Training\n",
    "\n",
    "Fit the data to the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X=X2_train, y=y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# pickle.dump(obj=model2, file=open('./models/final_regressor.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "final_regressor = pickle.load(file=open('./models/final_regressor.p', 'rb'))"
   ]
  },
  {
   "source": [
    "### Model Scoring\n",
    "\n",
    "Use the model to generate predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = final_regressor.predict(X2_test)\n",
    "y_pred_2"
   ]
  },
  {
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluate the performance of the final model based on standard regression metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regression metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_report(y_true, y_pred, type: str):\n",
    "    print(\n",
    "f'''Regression Report ({type})\n",
    "================================\n",
    "MSE:\\t\\t{np.round(mean_squared_error(y_true=y_true, y_pred=y_pred), 2)}\n",
    "MAE:\\t\\t{np.round(mean_absolute_error(y_true=y_true, y_pred=y_pred), 2)}\n",
    "R2:\\t\\t{np.round(r2_score(y_true=y_true, y_pred=y_pred), 4)}\n",
    "''')"
   ]
  },
  {
   "source": [
    "#### Evaluate against training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report(y2_train, final_regressor.predict(X2_train), type='train')"
   ]
  },
  {
   "source": [
    "#### Evaluate against testing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report(y2_test, y_pred_2, type='test')"
   ]
  },
  {
   "source": [
    "#### Evaluate against entire dataset (visualization)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_whole_2():\n",
    "    df2_whole = pd.read_csv('./data/kc_house_data.csv')\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(14, 8))\n",
    "\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            df_tmp = df2_whole.sample(frac=1)\n",
    "            scores = []\n",
    "            buffs = np.arange(50, 1000, 50)\n",
    "            for buf in buffs:\n",
    "                scores.append(final_regressor.score(df_tmp.drop('price', axis=1).iloc[:buf,:], df_tmp['price'].iloc[:buf]))\n",
    "            sns.lineplot(x=buffs, y=scores, color='black', ax=ax[r,c])\n",
    "            ax[r,c].set_ylim(0.4, 1.05)\n",
    "            ax[r,c].set_yticks(ticks=np.arange(0.5, 1.05, 0.1))\n",
    "            ax[r,c].set_xticks(ticks=np.arange(0, 1001, 100))\n",
    "            sns.lineplot(x=[0, 1000], y=[1.0] * 2, color='green', ax=ax[r,c])\n",
    "            sns.lineplot(x=[0, 1000], y=[0.9] * 2, color='orange', ax=ax[r,c])\n",
    "            sns.lineplot(x=[0, 1000], y=[0.8] * 2, color='red', ax=ax[r,c])\n",
    "            sns.lineplot(x=[100] * 2, y=[0.4, 1.], color='grey', ax=ax[r,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_whole_2()"
   ]
  },
  {
   "source": [
    "## Conclusions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}