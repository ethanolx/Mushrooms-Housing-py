{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd006e46f44bb844bdd55de1af887732b14e875f5b0d2374a3f4b9484cbcb737e08",
   "display_name": "Python 3.8.5  ('.venv': pipenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "06e46f44bb844bdd55de1af887732b14e875f5b0d2374a3f4b9484cbcb737e08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "source": [
    "# AIML CA1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import General Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Dependencies\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Graphing Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Dependencies\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Preservation Dependency\n",
    "import pickle\n",
    "\n",
    "# Miscellaneous Dependencies\n",
    "from typing import Callable, Dict, Union    # static typing\n",
    "from warnings import filterwarnings         # warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide Warnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "source": [
    "## Part A > Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How is your prediction task defined? And what is the meaning of the\n",
    "output variable?\n",
    "\n",
    "```\n",
    "    The task is to predict if a mushroom of the agaricus lepiota family is edible or poisonous,\n",
    "    given its properties (i.e. cap-shape, odor, etc.)\n",
    "\n",
    "    The output variable is class, and its possible values carry the respective meanings:\n",
    "    'edible':       the mushroom is safe for consumption\n",
    "    'poisonous':    do not consume the mushroom\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Data\n",
    "\n",
    "Load data about edibility of gilled mushrooms of the agaricus lepiota family"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How do you represent your data as features?\n",
    "\n",
    "```\n",
    "    The features are represented as columns in a pandas DataFrame\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   Did you bring in any additional sources of data?\n",
    "\n",
    "```\n",
    "    No, no external data sources were used\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.extraction import extract_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_mushrooms() -> pd.DataFrame:\n",
    "    # Extract raw content of ./data/agaricus-lepiota.names file\n",
    "    metadata: str\n",
    "    with open('./data/agaricus-lepiota.names') as f:\n",
    "        metadata = f.read()\n",
    "\n",
    "    # Extract attributes from metadata\n",
    "    attrs = extract_attributes(metadata, r'7\\. Attribute Information:.*\\n((.|\\n)*)8\\. Missing')\n",
    "\n",
    "    # Extract column names to be used for dataframe\n",
    "    cols = attrs.keys()\n",
    "\n",
    "    # Create the dataframe from ./data/agaricus-lepiota.data file,\n",
    "    #   using column names derived from ./data/agaricus-lepiota.names file\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer='./data/agaricus-lepiota.data',\n",
    "        sep=',',\n",
    "        header=0,\n",
    "        names=cols\n",
    "    )\n",
    "\n",
    "    # Expand attribute codes to their full definitions\n",
    "    for col in cols:\n",
    "        df[col].replace(to_replace=attrs[col] ,inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_mushrooms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_whole = df.drop(labels='class', axis=1)\n",
    "y_whole = df['class']\n",
    "X_build, X_final, y_build, y_final = train_test_split(X_whole, y_whole, test_size=0.2, random_state=1)\n",
    "df_build = pd.concat(objs=(X_build, y_build), axis=1)"
   ]
  },
  {
   "source": [
    "#### Inspect Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect top 10 rows of the dataset\n",
    "df.head(n=10)"
   ]
  },
  {
   "source": [
    "#### Summarize Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect overview of the dataset\n",
    "df.info()"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   Did you process the features in any way?\n",
    "\n",
    "```\n",
    "    Yes, the features underwent (feature) selection and (one-hot) encoding\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Exploratory Data Analysis (EDA)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to keep track of variables to be removed\n",
    "drop_cols = []"
   ]
  },
  {
   "source": [
    "Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isna().sum(axis=0)\n",
    "\n",
    "# Note that stalk-root has missing attributes (denoted as 'missing')\n",
    "# In fact, approx. 31% of the records have missing data for stalk-root\n",
    "stalk_dist = df['stalk-root'].value_counts()\n",
    "print((stalk_dist / stalk_dist.sum()).round(2))\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols.append('stalk-root')"
   ]
  },
  {
   "source": [
    "Redundant Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unqiue counts of the individual features\n",
    "print(df.describe().transpose().sort_values(by='unique', ascending=False))\n",
    "\n",
    "# Note that veil-type has only one value,\n",
    "#   hence it is a redundant feature\n",
    "print('\\n', 'Unique values for \\'veil-type\\': ', pd.unique(df['veil-type'].values), sep='')\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols.append('veil-type')"
   ]
  },
  {
   "source": [
    "Inspect the distribution of the target variable (class: edible/poisonous)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import format_label\n",
    "def plot_1A():\n",
    "    global df\n",
    "    ax, fig = plt.subplots(figsize=(7, 5))\n",
    "    ax = sns.countplot(data=df, x='class', palette='deep')\n",
    "    ax.set_ylim(top=5000)\n",
    "    ax.set_title(label='General Data Distribution')\n",
    "    ax.set_ylabel(ylabel='Number of Records')\n",
    "    ax.set_yticklabels(labels=format_label(ax.get_yticks() / 1000, lambda s: f'{round(s)}k'))\n",
    "    ax.set_xlabel(xlabel='Type')\n",
    "    total_count = df.shape[0]\n",
    "    for p in ax.patches:\n",
    "        x = p.get_x()\n",
    "        y = p.get_height()\n",
    "        ax.annotate(text=f'{y} ({y/total_count*100:.1f}%)', xy=(x + 0.23, y + 70))\n",
    "\n",
    "plot_1A()"
   ]
  },
  {
   "source": [
    "Inspect correlation between the independent variables and the target variable (class)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_1B():\n",
    "    global df\n",
    "    for i in df.drop(labels='class', axis=1).columns.values:\n",
    "        fig, (corr_plot, freq_plot) = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "        ct = pd.crosstab(index=df['class'], columns=df[i])\n",
    "        distr = df.groupby(i).count().iloc[:,0]\n",
    "        proportion = ((ct.loc['edible']) / distr) - 0.5\n",
    "        corr = pd.DataFrame(proportion.reset_index())\n",
    "        sns.barplot(data=corr, x=i, y=0, ax=corr_plot, color='grey')\n",
    "        sns.countplot(data=df.sort_values(by=i), x=i, hue='class', hue_order=['edible', 'poisonous'], ax=freq_plot, palette='turbo')\n",
    "        fig.suptitle(t=f'{i.upper()}')\n",
    "        corr_plot.set_title(label='Correlation (chi2-based)')\n",
    "        corr_plot.set_ylim((-0.6, 0.6))\n",
    "        corr_plot_x_lim = corr_plot.get_xlim()\n",
    "        corr_plot.set_yticks(ticks=np.arange(-0.5, 0.6, 0.1))\n",
    "        corr_plot.set_yticklabels(labels=np.round(corr_plot.get_yticks() + 0.5, 1))\n",
    "        corr_plot.set_ylabel(ylabel='Correlation')\n",
    "        corr_plot.set_xticklabels(labels=corr_plot.get_xticklabels(), rotation=30)\n",
    "        corr_plot.annotate(text='Tends to be \\'edible\\'', xy=((corr_plot_x_lim[0] + corr_plot_x_lim[1]) / 2, 0.55), ha='center', color='green')\n",
    "        corr_plot.annotate(text='Tends to be \\'poisonous\\'', xy=((corr_plot_x_lim[0] + corr_plot_x_lim[1]) / 2, -0.55), ha='center', color='orange')\n",
    "        freq_plot.set_title(label=f'Frequency Distribution')\n",
    "        freq_plot.set_xticklabels(labels=freq_plot.get_xticklabels(), rotation=30)\n",
    "\n",
    "plot_1B()"
   ]
  },
  {
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "There is no need for feature engineering in this dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Feature Selection\n",
    "\n",
    "There are 2 columns to be removed (stalk-root, veil-type)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns\n",
    "df.drop(labels=drop_cols, axis=1, inplace=True)\n",
    "df_build.drop(labels=drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "### Encoding the data\n",
    "\n",
    "The data has only categorical text variables, therefore they<br>have to be converted to numeric form using dummy variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (One-Hot) Encode the dataset (categorical -> binary)\n",
    "df_build = pd.get_dummies(data=df_build, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all the possible feature values are accounted for\n",
    "print(f'Shape of data subset:\\t\\t{df_build.shape}')\n",
    "print(f'Shape of entire dataset:\\t{pd.get_dummies(data=df, drop_first=True).shape}')"
   ]
  },
  {
   "source": [
    "### Inspect correlation after encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correlation between top 10 factors and target variable (class)\n",
    "df_build.corr()['class_poisonous'].drop(labels='class_poisonous').sort_values(key=lambda x: np.abs(x), ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi2-based feature selection\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Get top 10 factors that are correlated with the target variable (class)\n",
    "best_features_chi2 = SelectKBest(score_func=chi2, k=10).fit(X=df_build.drop(labels='class_poisonous', axis=1), y=df_build['class_poisonous'])\n",
    "best_features_mask = best_features_chi2.get_support()\n",
    "best_features = df_build.drop(labels='class_poisonous', axis=1).columns.values[best_features_mask]\n",
    "best_features_scores = best_features_chi2.scores_[best_features_mask]\n",
    "good_predictors = pd.Series(data=best_features_scores, index=best_features)\n",
    "\n",
    "good_predictors.sort_values(ascending=False)"
   ]
  },
  {
   "source": [
    "### Data Partitioning\n",
    "\n",
    "Split the data randomly into a train set and a test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X = df_build.drop(labels='class_poisonous', axis=1)\n",
    "y = df_build['class_poisonous']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "source": [
    "### Algorithm Selection & Hyper-Parameter Tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Determine best candidate algorithm using GridSearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How did you select which learning algorithms to use?\n",
    "\n",
    "```\n",
    "    GridSearchCV was used, and the algorithm used was one of the parameters.\n",
    "    For each algorithm, 6 different combinations of key hyperparameters were tested.\n",
    "    Out of all the combinations, the best performing algorithm was selected.\n",
    "    In this case, the algorithm is logistic regression.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate classification algorithms\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_clf():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('clf', DummyEstimator())\n",
    "    ])\n",
    "\n",
    "    params = [\n",
    "        {\n",
    "            'clf': [KNeighborsClassifier()],\n",
    "            'clf__n_neighbors': np.arange(3, 14, 2)\n",
    "        },\n",
    "        {\n",
    "            'clf': [LogisticRegression()],\n",
    "            'clf__solver': ['liblinear', 'newton-cg'],\n",
    "            'clf__C': np.logspace(-3, 3, 3),\n",
    "            'clf__multi_class': ['ovr']\n",
    "        },\n",
    "        {\n",
    "            'clf': [CategoricalNB()],\n",
    "            'clf__alpha': np.logspace(-3, 3, 6)\n",
    "        },\n",
    "        {\n",
    "            'clf': [SVC()],\n",
    "            'clf__kernel': ['rbf', 'linear'],\n",
    "            'clf__C': np.logspace(-3, 4, 3)\n",
    "        },\n",
    "        {\n",
    "            'clf': [DecisionTreeClassifier()],\n",
    "            'clf__max_depth': [10, 20, 30],\n",
    "            'clf__min_samples_leaf': [10, 30]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    best_clf_algo = GridSearchCV(estimator=pipe, param_grid=params, cv=3)\n",
    "    best_clf_algo.fit(X=X, y=y)\n",
    "    return best_clf_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=grid_search_clf(), file=open('./models/best_clf_algo.p', 'wb'))\n",
    "\n",
    "# Load result\n",
    "best_clf_algo_loaded = pickle.load(file=open('./models/best_clf_algo.p', 'rb'))\n",
    "\n",
    "# Inspect result\n",
    "print(best_clf_algo_loaded.best_estimator_)\n",
    "gs_clf = pd.DataFrame(best_clf_algo_loaded.cv_results_)\n",
    "gs_clf.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "#### Determine best hyperparameters for selected algorithm using GridSearch\n",
    "\n",
    "Selected Algorithm: Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   Did you try to tune the hyper parameters of the learning algorithm, and in that case how?\n",
    "\n",
    "```\n",
    "    Yes. GridSearchCV was used once again, but since the logistic regression algorithm\n",
    "    had already been selected, more hyperparameters specific to logistic regression can be tested.\n",
    "    \n",
    "    Standardization was also compared against no scaler.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_clf_params():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('scaler', DummyScaler()),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    params = {\n",
    "        'scaler': ['passthrough', StandardScaler()],\n",
    "        'clf__solver': ['liblinear', 'saga'],\n",
    "        'clf__tol': np.logspace(-5, 2, 3),\n",
    "        'clf__C': np.logspace(-4, 4, 5),\n",
    "        'clf__multi_class': ['ovr']\n",
    "    }\n",
    "\n",
    "    best_clf_params = GridSearchCV(estimator=pipe, param_grid=params, cv=5, n_jobs=-1)\n",
    "    best_clf_params.fit(X=X, y=y)\n",
    "    return best_clf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=grid_search_clf_params(), file=open('./models/best_clf_params.p', 'wb'))\n",
    "\n",
    "# Load result\n",
    "best_clf_params = pickle.load(file=open('./models/best_clf_params.p', 'rb'))\n",
    "\n",
    "# Inspect result\n",
    "print(best_clf_params.best_params_)\n",
    "gs_clf_params = pd.DataFrame(best_clf_params.cv_results_)\n",
    "gs_clf_params.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "### Combining Everything\n",
    "\n",
    "<a id=\"classification-finale\" /></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Building Pipeline\n",
    "<br>\n",
    "Build a machine learning pipeline, using\n",
    "\n",
    "*   a custom feature-selection transformer,\n",
    "*   a one-hot encoder,\n",
    "*   the most consistent algorithm,\n",
    "*   the best performing hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_cols_1(df: pd.DataFrame):\n",
    "    return df.drop(labels=drop_cols, axis=1, errors='ignore')\n",
    "\n",
    "def ensure_target_absence_1(df: pd.DataFrame):\n",
    "    return df.drop(labels='class', axis=1)\n",
    "\n",
    "class FeatureSelector1(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.has_target_variable: bool = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if 'class' in X.columns.values:\n",
    "            self.has_target_variable = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = drop_redundant_cols_1(X)\n",
    "        if self.has_target_variable:\n",
    "            X_copy = ensure_target_absence_1(X_copy)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Specify all possible column values for dataset\n",
    "def get_column_values_1(df: pd.DataFrame):\n",
    "    categories = []\n",
    "    for i in df.drop(labels=['class', *drop_cols], axis=1, errors='ignore').columns.values:\n",
    "        categories.append(pd.unique(df[i]))\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[\n",
    "    ('feature_selector', FeatureSelector1()),\n",
    "    ('encoder', OneHotEncoder(drop='first', categories=get_column_values_1(df))),\n",
    "    ('classifier', LogisticRegression(C=100.0, multi_class='ovr', solver='liblinear', tol=1e-05))\n",
    "])"
   ]
  },
  {
   "source": [
    "### Model Training\n",
    "\n",
    "Fit the data to the pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X=X_build, y=y_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# pickle.dump(obj=model, file=open('./models/final_classifier.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "final_classifier = pickle.load(file=open('./models/final_classifier.p', 'rb'))"
   ]
  },
  {
   "source": [
    "### Model Scoring\n",
    "\n",
    "Use the model to generate predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_classifier.predict(X=X_final)\n",
    "y_pred"
   ]
  },
  {
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluate the performance of the final model based on standard classification metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How do you evaluate the quality of your system?\n",
    "\n",
    "```\n",
    "    The machine learning pipeline was evaluated against:\n",
    "    1.  the train set,\n",
    "    2.  the test set,\n",
    "    3.  the entire dataset\n",
    "\n",
    "    in terms of:\n",
    "    1.  accuracy,\n",
    "    2.  precision,\n",
    "    3.  recall,\n",
    "    4.  f1-score\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model evaluation dependencies\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "source": [
    "#### Evaluate against build set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_build_pred = final_classifier.predict(X=X_build)\n",
    "\n",
    "# Classification summary\n",
    "print(classification_report(y_true=y_build, y_pred=y_build_pred, target_names=['edible', 'poisonous']))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\n', pd.DataFrame(data=confusion_matrix(y_true=y_build, y_pred=y_build_pred, labels=['edible', 'poisonous']), index=['Actual Edible', 'Actual Poisonous'], columns=['Predicted Edible', 'Predicted Poisonous']), '\\n\\n', sep='')\n",
    "\n",
    "# Print target distribution in y_build\n",
    "print(y_build.groupby(y_build).count())"
   ]
  },
  {
   "source": [
    "#### Evaluate against final set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How do you evaluate the quality of your system?\n",
    "\n",
    "```\n",
    "    To check for overfitting, the accuracy score for the build and final sets were compared.\n",
    "    They were approximately within 3% of each other (~0.99 and ~0.98)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How well does your system compare to a stupid baseline?\n",
    "\n",
    "```\n",
    "    My machine learning pipeline predicts correctly (~0.99) almost\n",
    "    twice as frequently as compared to a stupid baseline (~0.50)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   Can you say anything about the errors that the system makes? For a classification task, you may consider a confusion matrix.\n",
    "\n",
    "```\n",
    "    My machine learning pipeline is quite accurate, so there are few to no errors.\n",
    "    Using the confusion matrix as seen below, all the samples (mushrooms) in the test set\n",
    "    were correctly categorised as 'edible' or 'poisonous' respectively.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification summary\n",
    "print(classification_report(y_true=y_final, y_pred=y_pred, target_names=['edible', 'poisonous']))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\n', pd.DataFrame(data=confusion_matrix(y_true=y_final, y_pred=y_pred, labels=['edible', 'poisonous']), index=['Actual Edible', 'Actual Poisonous'], columns=['Predicted Edible', 'Predicted Poisonous']), '\\n\\n', sep='')\n",
    "\n",
    "# Print target distribution in y_final\n",
    "print(y_final.groupby(y_final).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy baseline for classification\n",
    "from sklearn.dummy import DummyClassifier\n",
    "dummy = DummyClassifier(strategy='uniform')\n",
    "dummy.fit(X_build, y_build)\n",
    "dummy.score(X_final, y_final)"
   ]
  },
  {
   "source": [
    "#### Evaluate against entire dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_whole_pred = final_classifier.predict(X=X_whole)\n",
    "\n",
    "# Classification summary\n",
    "print(classification_report(y_true=y_whole, y_pred=y_whole_pred, target_names=['edible', 'poisonous']))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\n', pd.DataFrame(data=confusion_matrix(y_true=y_whole, y_pred=y_whole_pred, labels=['edible', 'poisonous']), index=['Actual Edible', 'Actual Poisonous'], columns=['Predicted Edible', 'Predicted Poisonous']), '\\n\\n', sep='')\n",
    "\n",
    "# Print target distribution in y_whole\n",
    "print(y_whole.groupby(y_whole).count())"
   ]
  },
  {
   "source": [
    "*   Is it possible to say something about which features the model considers\n",
    "important? (Whether this is possible depends on the type of classifier\n",
    "you are using)\n",
    "\n",
    "```\n",
    "    The model does produce numerical coefficients, but these are not directly indicative of the\n",
    "    feature importances.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_coefs_1():\n",
    "    global df, final_classifier\n",
    "    coefs = final_classifier['classifier'].coef_.reshape((-1,))\n",
    "    final_features = pd.get_dummies(FeatureSelector1().fit_transform(df), drop_first=True).columns.values\n",
    "    feature_coefs = pd.Series(data=coefs, index=final_features)\n",
    "    return feature_coefs\n",
    "\n",
    "get_feature_coefs_1().sort_values(ascending=False, key=lambda x: np.abs(x))"
   ]
  },
  {
   "source": [
    "## Part B > Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How is your prediction task defined? And what is the meaning of the output variable?\n",
    "\n",
    "```\n",
    "    The task is to predict the selling price of a house,\n",
    "    given its properties (i.e. number of bedrooms, floor area, etc.)\n",
    "\n",
    "    The output variable is price in USD, and is the predicted price of a house\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Data\n",
    "\n",
    "Load data about King County house sales"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How do you represent your data as features?\n",
    "\n",
    "```\n",
    "    The features are represented as columns in a pandas DataFrame\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   Did you bring in any additional sources of data?\n",
    "\n",
    "```\n",
    "    No, no external sources of data were used\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from a csv file\n",
    "df2 = pd.read_csv('./data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_whole = df2.drop(labels='price', axis=1)\n",
    "y2_whole = df2['price']\n",
    "X2_build, X2_final, y2_build, y2_final = train_test_split(X2_whole, y2_whole, test_size=0.2, random_state=4)\n",
    "df2_build = pd.concat(objs=(X2_build, y2_build), axis=1)"
   ]
  },
  {
   "source": [
    "#### Inspect Data\n",
    "\n",
    "Preview a sample of the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the top 10 rows of the dataset\n",
    "df2.head(n=10)"
   ]
  },
  {
   "source": [
    "#### Summarize Data\n",
    "\n",
    "Get a sense of the features involved"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect overview of the dataset\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect statistics of the dataset\n",
    "df2.describe().transpose().round(2)"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   Did you process the features in any way?\n",
    "\n",
    "```\n",
    "    Yes, the features underwent (feature) engineering and selection,\n",
    "    logarithmic transformation, and standardization.\n",
    "\n",
    "    In addition, the target also underwent logarithmic transformation.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Exploratory Data Analysis (EDA)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to keep track of variables to be removed\n",
    "drop_cols_2 = []\n",
    "\n",
    "# List to keep track of positively skewed variables\n",
    "positively_skewed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df2.isna().sum(axis=0)\n",
    "\n",
    "# There doesn't seem to be any missing values"
   ]
  },
  {
   "source": [
    "Visualize correlation amongst the original features using a heatmap"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2A():\n",
    "    global df2\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(data=df2.corr(), cmap='RdBu', vmin=-1, vmax=1, ax=ax)\n",
    "    ax.set_title(label='Correlation Matrix')\n",
    "plot_2A()"
   ]
  },
  {
   "source": [
    "Inspect distribution of the individual variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2B():\n",
    "    global df2\n",
    "    for i in df2.columns.values:\n",
    "        if df2[i].dtype.kind in 'biufc':\n",
    "            with sns.axes_style(style='darkgrid'):\n",
    "                fig, (hst, bxp) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "                fig.suptitle(i.upper())\n",
    "                sns.histplot(data=df2, x=i, ax=hst, palette='deep')\n",
    "                sns.boxplot(data=df2, x=i, ax=bxp, palette='deep')\n",
    "plot_2B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many of the features seem to be positively skewed\n",
    "\n",
    "# Course of action - log/sqrt transformation\n",
    "positively_skewed.extend(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'long', 'sqft_living15', 'sqft_lot15'])"
   ]
  },
  {
   "source": [
    "Inspect correlation between the features and the target variable (price)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2C():\n",
    "    global df2, positively_skewed\n",
    "    for i in positively_skewed:\n",
    "        perc = df2[df2[i] == 0].shape[0] / df.shape[0] * 100\n",
    "        fig, (non, log, sqrt) = plt.subplots(ncols=3, figsize=(8, 3))\n",
    "        non.set_title(label='No Transformation')\n",
    "        log.set_title(label='After Log Transformation')\n",
    "        sqrt.set_title(label='After Sqrt Transformation')\n",
    "        sns.histplot(x=df2[i], ax=non)\n",
    "        sns.histplot(x=np.log1p(df2[i]), ax=log)\n",
    "        sns.histplot(x=np.sqrt(df2[i]), ax=sqrt)\n",
    "plot_2C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr()['price'].sort_values(key=lambda x: np.abs(x), ascending=False)"
   ]
  },
  {
   "source": [
    "Inspect id feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check id data type\n",
    "print('ID data type:\\t\\t', df2['id'].dtype)\n",
    "\n",
    "# Compare the number of ids to the total number of records \n",
    "print('Number of unique IDs:\\t', pd.unique(df2['id']).size)\n",
    "print('Total no. of records:\\t', df2.shape[0], '\\n')\n",
    "\n",
    "# Check correlation between id and the rest of the variables\n",
    "print(df2.corr()['id'].sort_values(key=lambda x: np.abs(x), ascending=False))\n",
    "\n",
    "\n",
    "# id seems redundant\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols_2.append('id')"
   ]
  },
  {
   "source": [
    "Inspect zipcode feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check zipcode data type\n",
    "print('zipcode data type:\\t\\t', df2['zipcode'].dtype)\n",
    "\n",
    "# Compare the number of zipcodes to the total number of records \n",
    "print('Number of unique zipcodes:\\t', pd.unique(df2['zipcode']).size)\n",
    "print('Total no. of records:\\t', df2.shape[0], '\\n')\n",
    "\n",
    "# Check correlation between zipcode and the rest of the variables\n",
    "print(df2.corr()['zipcode'].sort_values(key=lambda x: np.abs(x), ascending=False))\n",
    "\n",
    "\n",
    "# zipcode does not seem redundant\n",
    "\n",
    "# Course of action - no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D():\n",
    "    global df2, positively_skewed\n",
    "    for i in positively_skewed:\n",
    "        with sns.axes_style(style='whitegrid'):\n",
    "            fig, (bef, log, sqrt) = plt.subplots(ncols=3, figsize=(10, 5))\n",
    "            bef.set_title(label='No Transformation')\n",
    "            log.set_title(label='Log Transformation')\n",
    "            sqrt.set_title(label='Sqrt Transformation')\n",
    "            sns.scatterplot(data=df2, x=i, y='price', ax=bef)\n",
    "            sns.scatterplot(x=np.log1p(df2[i]), y=df2['price'], ax=log)\n",
    "            sns.scatterplot(x=np.sqrt(df2[i]), y=df2['price'], ax=sqrt)\n",
    "plot_2D()"
   ]
  },
  {
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "There seems to be useful extractable data in the `date` feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month and day from the date feature\n",
    "df2_date = pd.to_datetime(df2['date'], yearfirst=True)\n",
    "df2['year'] = pd.DatetimeIndex(data=df2_date).year\n",
    "df2['month'] = pd.DatetimeIndex(data=df2_date).month\n",
    "df2['day'] = pd.DatetimeIndex(data=df2_date).day\n",
    "\n",
    "df2_build_date = pd.to_datetime(df2_build['date'], yearfirst=True)\n",
    "df2_build['year'] = pd.DatetimeIndex(data=df2_build_date).year\n",
    "df2_build['month'] = pd.DatetimeIndex(data=df2_build_date).month\n",
    "df2_build['day'] = pd.DatetimeIndex(data=df2_build_date).day\n",
    "\n",
    "# Date variable seems redundant now\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols_2.append('date')"
   ]
  },
  {
   "source": [
    "#### Feature Selection\n",
    "\n",
    "Drop redundant columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review columns to be dropped\n",
    "drop_cols_2"
   ]
  },
  {
   "source": [
    "There are 2 columns to be removed (`id`, `date`)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns\n",
    "df2.drop(labels=drop_cols_2, axis=1, inplace=True)\n",
    "df2_build.drop(labels=drop_cols_2, axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "### Data Partitioning\n",
    "\n",
    "Split the data randomly into a train set and a test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X2 = df2_build.drop(labels='price', axis=1)\n",
    "y2 = df2_build['price']\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=1)"
   ]
  },
  {
   "source": [
    "### Algorithm Selection & Hyper-Parameter Tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Determine best regression algorithm using GridSearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How did you select which learning algorithms to use?\n",
    "\n",
    "```\n",
    "    GridSearchCV was used, and the algorithm used was one of the parameters.\n",
    "    For each algorithm, a maximum of 6 different combinations of key hyperparameters were tested.\n",
    "    Out of all the combinations, the best performing algorithm was selected.\n",
    "    In this case, the algorithm is gradient boosting regressor.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate regression algorithms\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_reg():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', DummyEstimator())\n",
    "    ])\n",
    "\n",
    "    params = [\n",
    "        {\n",
    "            'reg': [LinearRegression()],\n",
    "            'reg__normalize': [True, False],\n",
    "            'reg__fit_intercept': [True, False]\n",
    "        },\n",
    "        {\n",
    "            'reg': [Lasso(), Ridge()],\n",
    "            'reg__alpha': np.logspace(-5, 3, 6)\n",
    "        },\n",
    "        {\n",
    "            'reg': [DecisionTreeRegressor(), GradientBoostingRegressor()],\n",
    "            'reg__max_depth': np.arange(5, 11)\n",
    "        },\n",
    "        {\n",
    "            'reg': [KNeighborsRegressor()],\n",
    "            'reg__n_neighbors': np.arange(5, 11)\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    best_reg_algo = GridSearchCV(estimator=pipe, param_grid=params, cv=3, n_jobs=-1)\n",
    "    best_reg_algo.fit(X=X2, y=y2)\n",
    "    return best_reg_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=grid_search_reg(), file=open('./models/best_reg_algo.p', 'wb'))\n",
    "\n",
    "# Inspect result\n",
    "best_reg_algo_loaded = pickle.load(file=open('./models/best_reg_algo.p', 'rb'))\n",
    "\n",
    "print(best_reg_algo_loaded.best_estimator_)\n",
    "gs_reg = pd.DataFrame(best_reg_algo_loaded.cv_results_)\n",
    "gs_reg.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "#### Determine best hyperparameters for selected algorithm using GridSearch\n",
    "\n",
    "Selected Algorithm: Gradient Boosting Regressor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   Did you try to tune the hyper parameters of the learning algorithm, and\n",
    "in that case how?\n",
    "\n",
    "```\n",
    "    Yes. GridSearchCV was used once again, but since the gradient boosting regressor algorithm\n",
    "    had already been selected, more hyperparameters specific to gradient boosting regressor can be tested.\n",
    "    \n",
    "    In this case, max depth, min samples for leaves and for splits were tested.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_reg_params():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', GradientBoostingRegressor())\n",
    "    ])\n",
    "\n",
    "    params = {\n",
    "        'reg__max_depth': np.arange(2, 5),\n",
    "        'reg__min_samples_split': np.arange(9, 100, 30),\n",
    "        'reg__min_samples_leaf': np.arange(9, 100, 30)\n",
    "    }\n",
    "\n",
    "    best_reg_params = GridSearchCV(estimator=pipe, param_grid=params, cv=5, n_jobs=-1)\n",
    "    best_reg_params.fit(X=X2, y=y2)\n",
    "    return best_reg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=grid_search_reg_params(), file=open('./models/best_reg_params.p', 'wb'))\n",
    "\n",
    "# Inspect result\n",
    "best_reg_params = pickle.load(file=open('./models/best_reg_params.p', 'rb'))\n",
    "\n",
    "print(best_reg_params.best_params_)\n",
    "gs_reg_params = pd.DataFrame(best_reg_params.cv_results_)\n",
    "gs_reg_params.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "Further tuning - Scaler, Normalizing X Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        all_cols = X.columns.values\n",
    "        for i in positively_skewed:\n",
    "            if i != 'long':\n",
    "                X_copy[i] = np.log1p(X[i])\n",
    "        return X_copy\n",
    "\n",
    "class SqrtTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        all_cols = X.columns.values\n",
    "        for i in positively_skewed:\n",
    "            if i != 'long':\n",
    "                X_copy[i] = np.sqrt(X[i])\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_transformer():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('trans', DummyTransformer()),\n",
    "        ('scaler', DummyScaler()),\n",
    "        ('reg', GradientBoostingRegressor(max_depth=4, min_samples_leaf=9, min_samples_split=9))\n",
    "    ])\n",
    "\n",
    "    params = {\n",
    "        'trans': ['passthrough', LogTransformer(), SqrtTransformer()],\n",
    "        'scaler': ['passthrough', StandardScaler(), RobustScaler()]\n",
    "    }\n",
    "\n",
    "    best_trans_params = GridSearchCV(estimator=pipe, param_grid=params, cv=5, n_jobs=-1)\n",
    "    best_trans_params.fit(X=X2, y=y2)\n",
    "    return best_trans_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=best_transformer(), file=open('./models/best_reg_trans.p', 'wb'))\n",
    "\n",
    "# Inspect result\n",
    "best_reg_trans = pickle.load(file=open('./models/best_reg_trans.p', 'rb'))\n",
    "\n",
    "print(best_reg_trans.best_params_)\n",
    "gs_reg_trans = pd.DataFrame(best_reg_trans.cv_results_)\n",
    "gs_reg_trans.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "Further tuning - Normalizing y Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def further_tune_reg(cv: int = 4):\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('trans', LogTransformer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', GradientBoostingRegressor(max_depth=4, min_samples_leaf=9, min_samples_split=9))\n",
    "    ])\n",
    "\n",
    "    sqrt_y = TransformedTargetRegressor(regressor=pipe, func=np.sqrt, inverse_func=np.square)\n",
    "\n",
    "    log_y = TransformedTargetRegressor(regressor=pipe, func=np.log1p, inverse_func=np.expm1)\n",
    "\n",
    "    scores = []\n",
    "    for m in (pipe, sqrt_y, log_y):\n",
    "        scores.append(cross_val_score(estimator=m, X=X2, y=y2, cv=cv, randomr))\n",
    "    result = pd.DataFrame(data=scores, columns=[f'Test {i + 1}' for i in range(cv)], index=['no y transformation', 'sqrt y transformation', 'log y transformation'])\n",
    "    result['Mean Score'] = result.mean(axis=1)\n",
    "    result['Std Score'] = result.std(axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=further_tune_reg(), file=open('./models/best_reg_y_trans.p', 'wb'))\n",
    "\n",
    "# Inspect result\n",
    "best_reg_y_trans = pickle.load(file=open('./models/best_reg_y_trans.p', 'rb'))\n",
    "\n",
    "print(best_reg_y_trans.sort_values(by='Mean Score', ascending=False))"
   ]
  },
  {
   "source": [
    "### Combining Everything\n",
    "\n",
    "<a id=\"regression-finale\" /></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Building the pipeline\n",
    "<br>\n",
    "Build the machine learning pipeline, using\n",
    "\n",
    "*   a custom feature-engineering transformer,\n",
    "*   a custom feature-selection transformer,\n",
    "*   a custom logarithmic transformer,\n",
    "*   a standard scaler,\n",
    "*   the most consistent algorithm (gradient boosting regressor),\n",
    "*   the best performing hyperparameters\n",
    "\n",
    "To further improve performance and reduce overfitting,<br>\n",
    "the target variable will be transformed too (sqrt)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_parts(df: pd.DataFrame, col: str, **kwargs):\n",
    "    df_datetime = pd.DatetimeIndex(df[col], **kwargs)\n",
    "    return df_datetime.year, df_datetime.month, df_datetime.day\n",
    "\n",
    "class FeatureEngineer2(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_year, X_month, X_day = extract_date_parts(df=X, col='date', yearfirst=True)\n",
    "        X_copy['year'] = X_year\n",
    "        X_copy['month'] = X_month\n",
    "        X_copy['day'] = X_day\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_cols_2(df: pd.DataFrame):\n",
    "    return df.drop(labels=['id', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "def ensure_target_absence_2(df: pd.DataFrame):\n",
    "    return df.drop(labels='price', axis=1)\n",
    "\n",
    "class FeatureSelector2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.has_target_variable: bool = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if 'price' in X.columns.values:\n",
    "            self.has_target_variable = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = drop_redundant_cols_2(X)\n",
    "        if self.has_target_variable:\n",
    "            X_copy = ensure_target_absence_2(X_copy)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipe2 = Pipeline(steps=[\n",
    "    ('feature_engineer', FeatureEngineer2()),\n",
    "    ('feature_selector', FeatureSelector2()),\n",
    "    ('sqrt_transformer', LogTransformer()),\n",
    "    ('standard_scaler', StandardScaler()),\n",
    "    ('regressor', GradientBoostingRegressor(max_depth=4, min_samples_leaf=9, min_samples_split=9))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap pipeline in a target transformer\n",
    "model2 = TransformedTargetRegressor(regressor=pipe2, func=np.log1p, inverse_func=np.expm1, check_inverse=False)"
   ]
  },
  {
   "source": [
    "### Model Training\n",
    "\n",
    "Fit the data to the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X=X2_build, y=y2_build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "# pickle.dump(obj=model2, file=open('./models/final_regressor.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "final_regressor = pickle.load(file=open('./models/final_regressor.p', 'rb'))"
   ]
  },
  {
   "source": [
    "### Model Scoring\n",
    "\n",
    "Use the model to generate predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = final_regressor.predict(X2_final)\n",
    "y_pred_2"
   ]
  },
  {
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluate the performance of the final model based on standard regression metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How do you evaluate the quality of your system?\n",
    "\n",
    "```\n",
    "    The machine learning pipeline was evaluated against:\n",
    "    1.  the train set,\n",
    "    2.  the test set,\n",
    "    3.  the entire dataset\n",
    "\n",
    "    in terms of:\n",
    "    1.  mean squared error,\n",
    "    2.  mean absolute error,\n",
    "    3.  r squared\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Import regression metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_report(y_true, y_pred, type: str):\n",
    "    print(\n",
    "f'''Regression Report ({type})\n",
    "================================\n",
    "MSE:\\t\\t{np.round(mean_squared_error(y_true=y_true, y_pred=y_pred), 2)}\n",
    "MAE:\\t\\t{np.round(mean_absolute_error(y_true=y_true, y_pred=y_pred), 2)}\n",
    "R2:\\t\\t{np.round(r2_score(y_true=y_true, y_pred=y_pred), 4)}\n",
    "''')"
   ]
  },
  {
   "source": [
    "#### Evaluate against build set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report(y2_build, final_regressor.predict(X2_build), type='train')"
   ]
  },
  {
   "source": [
    "#### Evaluate against final set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How do you evaluate the quality of your system?\n",
    "\n",
    "```\n",
    "    To check for overfitting, the r2 score for the build and final sets were compared.\n",
    "    They were approximately within 3% of each other (~0.90 and ~0.88)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   How well does your system compare to a stupid baseline?\n",
    "\n",
    "```\n",
    "    My machine learning pipeline predicts more accurately than a stupid baseline by a huge margin.\n",
    "\n",
    "    The r2 score (which describes how well the model fits the data) of my machine learning pipeline is ~0.86\n",
    "    while that of a stupid baseline is negative (absolutely bad fit with respect to the data).\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*   Can you say anything about the errors that the system makes?\n",
    "\n",
    "```\n",
    "    Concerning a regression problem, it is impossible to predict an exact value with machine learning.\n",
    "    Therefore, the metrics by which a regression machine learning model is evaluated take into account\n",
    "    the margin of error which the model makes. For example, r2 shows the proportion of variance\n",
    "    (between a predicted value and the actual value) attributed to variance in the feature values.\n",
    "    Hence, an r2 score close to 1 suggests success for a machine learning model.\n",
    "\n",
    "    In this case 0.88 for a test score is considerably good.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report(y2_final, y_pred_2, type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy baseline for regression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "dummy2 = DummyRegressor(strategy='median')\n",
    "dummy2.fit(X2_build, y2_build)\n",
    "dummy2.score(X2_final, y2_final)"
   ]
  },
  {
   "source": [
    "#### Evaluate against entire dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report(y2_whole, final_regressor.predict(X2_whole), type='entire')"
   ]
  },
  {
   "source": [
    "#### Evaluate against entire dataset (visualization)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2E():\n",
    "    df2_new = pd.read_csv('./data/kc_house_data.csv')\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(16, 10))\n",
    "\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            df_tmp = df2_new.sample(frac=1)\n",
    "            scores = []\n",
    "            buffers = np.arange(100, 1000, 50)\n",
    "            for buf in buffers:\n",
    "                scores.append(final_regressor.score(df_tmp.drop('price', axis=1).iloc[:buf,:], df_tmp['price'].iloc[:buf]))\n",
    "            sns.lineplot(x=buffers, y=scores, color='black', ax=ax[r,c])\n",
    "            ax[r,c].set(\n",
    "                title=f'Test {r * 3 + c + 1}',\n",
    "                ylim=(0.4, 1.05),\n",
    "                yticks=np.arange(0.5, 1.05, 0.1),\n",
    "                ylabel='r2',\n",
    "                xticks=np.arange(0, 1001, 100),\n",
    "                xlabel='Buffer Size'\n",
    "            )\n",
    "            sns.lineplot(x=[0, 1000], y=[1.0] * 2, color='green', ax=ax[r,c])\n",
    "            sns.lineplot(x=[0, 1000], y=[0.9] * 2, color='orange', ax=ax[r,c])\n",
    "            sns.lineplot(x=[0, 1000], y=[0.8] * 2, color='red', ax=ax[r,c])\n",
    "            sns.lineplot(x=[500] * 2, y=[0.4, 1.], color='grey', ax=ax[r,c])\n",
    "            \n",
    "plot_2E()"
   ]
  },
  {
   "source": [
    "*   Is it possible to say something about which features the model considers important?\n",
    "\n",
    "```\n",
    "    The more important features have greater values as seen below.\n",
    "    It seems grade, floor area and geographical coordinates are some of the more important features.\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances_2():\n",
    "    global X2_build, final_regressor\n",
    "    impts = final_regressor.regressor_['regressor'].feature_importances_\n",
    "    final_features = FeatureSelector2().fit_transform(FeatureEngineer2().fit_transform(X2_build)).columns.values\n",
    "    feature_impts = pd.Series(data=impts, index=final_features)\n",
    "    return feature_impts\n",
    "\n",
    "get_feature_importances_2().sort_values(ascending=False)"
   ]
  },
  {
   "source": [
    "## Conclusions\n",
    "\n",
    "Every machine learning problem is different, and there is no one best algorithm to solve all.\n",
    "It is the data scientist's responsibility to experiment and select the most appropriate algorithm that is able to generalise patterns in the available data, and make accurate and precise predictions on new unseen data. Nonetheless, that is just the beginning as there are hyperparameters to tune, transformations to be performed and overfitting to check for...\n",
    "\n",
    "In Part A, the [Logistic Regression algorithm](#classification-finale) was chosen, whereas in Part B, the [Gradient Boosting Regressor Ensemble algorithm](#regression-finale) was chosen instead. One-Hot Encoding was applied for Part A, while logarithmic and square root transformations were conducted for Part B."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}