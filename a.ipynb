{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd006e46f44bb844bdd55de1af887732b14e875f5b0d2374a3f4b9484cbcb737e08",
   "display_name": "Python 3.8.5 32-bit ('.venv': pipenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "06e46f44bb844bdd55de1af887732b14e875f5b0d2374a3f4b9484cbcb737e08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "source": [
    "# AIML CA1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import General Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Dependencies\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Graphing Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Dependencies\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Miscellaneous Dependencies\n",
    "from typing import Callable, Dict # static typing\n",
    "\n",
    "# Utility Functions\n",
    "from utils.extraction import extract_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "## Utility Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Part I"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Exclusive Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Classification Metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "source": [
    "### Import Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract raw content of ./data/agaricus-lepiota.names file\n",
    "metadata: str\n",
    "with open('./data/agaricus-lepiota.names') as f:\n",
    "    metadata = f.read()\n",
    "\n",
    "# Extract attributes from metadata\n",
    "attrs = extract_attributes(metadata, r'7\\. Attribute Information:.*\\n((.|\\n)*)8\\. Missing')\n",
    "\n",
    "# Extract column names to be used for dataframe\n",
    "cols = attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the dataframe from ./data/agaricus-lepiota.data file,\n",
    "#   using column names derived from ./data/agaricus-lepiota.names file\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='./data/agaricus-lepiota.data',\n",
    "    sep=',',\n",
    "    header=0,\n",
    "    names=cols\n",
    ")\n",
    "\n",
    "# Expand attribute codes to their full definitions\n",
    "for col in cols:\n",
    "    df[col].replace(to_replace=attrs[col] ,inplace=True)"
   ]
  },
  {
   "source": [
    "#### Inspect Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect top 10 rows of the dataset\n",
    "df.head(n=10)"
   ]
  },
  {
   "source": [
    "#### Summarize Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect overview of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect statistics of the dataset\n",
    "df.describe().transpose().sort_values(by='unique', ascending=False)\n",
    "\n",
    "# Note that veil-type has only one value,\n",
    "#   hence it is redundant\n",
    "df.drop(labels='veil-type', axis=1)"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isna().sum(axis=0)\n",
    "\n",
    "# Note that stalk-root has missing attributes (denoted as 'missing')\n",
    "df['stalk-root'] = df['stalk-root'].str.replace(pat='missing', repl='unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_A(df: pd.DataFrame):\n",
    "    for i in df.drop(labels='class', axis=1).columns.values:\n",
    "        fig, ax = plt.subplots(ncols=2)\n",
    "        expt = pd.crosstab(index=df['class'], columns=df[i])\n",
    "        dum = df.groupby(i).count().iloc[:,0]\n",
    "        perc = (expt.iloc[0] - expt.iloc[1]) / dum\n",
    "        o = pd.DataFrame(perc.reset_index())\n",
    "        sns.barplot(data=o, x=i, y=0, ax=ax[0], color='grey')\n",
    "        sns.countplot(data=df.sort_values(by=i), x=i, hue='class', ax=ax[1])\n",
    "plot_A(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_B(df: pd.DataFrame):\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#     _ = sns.countplot(data=df.sort_values('cap-color'), x='cap-color', hue='class', ax=ax)\n",
    "# plot_B(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe = pd.get_dummies(data=df, drop_first=True)\n",
    "df_ohe.corr()['class_poisonous'].sort_values(key=lambda x: np.abs(x), ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Check correlation between attributes\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     yoyo = pd.get_dummies(df, drop_first=True)\n",
    "#     df_corr = yoyo.corr()\n",
    "#     df_corr_targ = df_corr\n",
    "#     df_minor_mask = df_corr_targ.apply(func=lambda s: np.abs(s['class_poisonous']) > 0.5)\n",
    "#     df_corr_sort = df_corr_targ[df_minor_mask].sort_values(by='class_poisonous', key=lambda s: -np.abs(s))\n",
    "#     # df_corr_sort.drop(labels='class_poisonous', inplace=True)\n",
    "#     targeted_corr = df_corr.loc[df_corr_sort.index.values, df_corr_sort.index.values]\n",
    "#     sns.pairplot(targeted_corr, hue='class_poisonous', diag_kind=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi square\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# t = pd.get_dummies(df, drop_first=True).groupby(by='class_poisonous').sum().astype(int)\n",
    "t = pd.get_dummies(df).drop(labels='class_edible', axis=1)\n",
    "\n",
    "s = SelectKBest(score_func=chi2, k=10).fit(t.drop(labels='class_poisonous', axis=1), t[['class_poisonous']])\n",
    "good_preds = t.drop(labels='class_poisonous', axis=1).columns.values[s.get_support()]\n",
    "# sns.pairplot(data=t.drop(labels=t.columns.values[t.columns.values]))\n",
    "good_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdf = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import format_label\n",
    "def plot_A(df: pd.DataFrame):\n",
    "    ax = sns.countplot(data=df, x='class', palette='deep')\n",
    "    ax.set_ylim(top=5000)\n",
    "    ax.set_title(label='General Data Distribution')\n",
    "    ax.set_ylabel(ylabel='Number of Records')\n",
    "    ax.set_yticklabels(labels=format_label(\n",
    "        ax.get_yticks() / 1000, lambda s: f'{round(s)}k'))\n",
    "    ax.set_xlabel(xlabel='Type')\n",
    "    total_count = df.shape[0]\n",
    "    for p in ax.patches:\n",
    "        x = p.get_x()\n",
    "        y = p.get_height()\n",
    "        ax.annotate(text=f'{y} ({y/total_count*100:.1f}%)',\n",
    "                    xy=(x + 0.21, y + 70))\n",
    "    return ax\n",
    "ax_a = plot_A(df=df)\n",
    "ax_a"
   ]
  },
  {
   "source": [
    "### Data Partitioning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ohe.drop(labels='class_poisonous', axis=1)\n",
    "y = df_ohe['class_poisonous']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "source": [
    "### Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)\n",
    "# confusion_matrix(y_test, knn.predict(X_test))\n",
    "# knn.predict(X_test.iloc[0:1,:])\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neighbours_range = list(range(1, 20))\n",
    "# # fig, ax = plt.subplots(nrows=1, ncols=len(neighbours_range), sharey=True)\n",
    "# result = np.empty(shape=(0,))\n",
    "# for neighbours in neighbours_range:\n",
    "#     cv = cross_val_score(estimator=KNeighborsClassifier(n_neighbors=neighbours), X=X, y=y, cv=8)\n",
    "#     # print(f\"Neighbours: {neighbours}\\t| Mean: {cv.mean()}\\t| Median: {np.median(cv)}\")\n",
    "#     result = np.hstack((result, np.array([cv.mean()])))\n",
    "#     # sns.swarmplot(y=cv, ax=ax[neighbours - 3])\n",
    "\n",
    "# sns.lineplot(x=neighbours_range, y=result)\n",
    "# print(result)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('clf', DummyEstimator())\n",
    "# ])\n",
    "\n",
    "# params = [\n",
    "#     {\n",
    "#         'clf': [KNeighborsClassifier()],\n",
    "#         'clf__n_neighbors': np.arange(start=4, stop=10)\n",
    "#     },\n",
    "#     {\n",
    "#         'clf': [LogisticRegression(solver='newton-cg')],\n",
    "#         'clf__C': np.logspace(-1, 2, 3)\n",
    "#     },\n",
    "#     {\n",
    "#         'clf': [GaussianNB()]\n",
    "#     },\n",
    "#     {\n",
    "#         'clf': [SVC()],\n",
    "#         'clf__C': np.logspace(-1, 2, 3)\n",
    "#     },\n",
    "#     {\n",
    "#         'clf': [DecisionTreeClassifier()],\n",
    "#         'clf__max_depth': [10, 20, 30]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# cv = GridSearchCV(estimator=pipeline, param_grid=params, cv=5)\n",
    "# cv.fit(X=X, y=y)\n",
    "# import pickle\n",
    "# pickle.dump(obj=cv, file=open(\"./models/grid_search_clf.p\", \"wb\"))\n",
    "# print(cv.best_params_)\n",
    "# print(cv.best_score_)\n",
    "# print(cv.best_estimator_)\n",
    "# pd.DataFrame(data=cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "mod = pickle.load(file=open('./models/grid_search_clf.p', 'rb'))\n",
    "df_results = pd.DataFrame(mod.cv_results_)\n",
    "df_results.to_csv('./data/a.csv')\n",
    "df_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = df_results.copy()\n",
    "t_d['param_clf'].astype('str')\n",
    "# ddd = t_d.groupby('param_clf').mean()\n",
    "# try_out = ddd.reset_index().melt(id_vars='param_clf', var_name='test', value_name='s')\n",
    "# try_out['test'] = try_out['test'].str.slice(5, 6).astype(int)\n",
    "# try_out['test'] = try_out['test'].str.extract(pat=r'*([\\d])*', expand=False)\n",
    "# sns.lineplot(data=try_out, x='test', y='s', hue='param_clf')\n",
    "# try_out\n",
    "\n",
    "fig, ax = plt.subplots(ncols=5, sharey=True, figsize=(12, 8))\n",
    "hyp = ['param_clf__n_neighbors', 'param_clf__C', None, 'param_clf__C', 'param_clf__max_depth']\n",
    "for i, est in enumerate(pd.unique(t_d['param_clf'])):\n",
    "    stuff = t_d[t_d['param_clf'] == est].melt(id_vars=['param_clf', 'param_clf__n_neighbors', 'param_clf__C', 'param_clf__max_depth'], value_vars=['split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score'], var_name='test', value_name='score')\n",
    "    stuff['test'] = stuff['test'].str.extract(pat='([\\\\d])', expand=False)\n",
    "    stuff['test'] = stuff['test'].astype(int)\n",
    "    stuff['test'] += 1\n",
    "    stuff.dropna(axis=1, inplace=True)\n",
    "    ax[i].set_ylim((0.5, 1.2))\n",
    "    ax[i].set_title(est)\n",
    "    ax[i].set_xticks(ticks=range(1, 6))\n",
    "    sns.lineplot(data=stuff, x='test', y='score', hue=hyp[i], ax=ax[i], palette='muted')"
   ]
  },
  {
   "source": [
    "## Part II"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Exclusive Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Models (Regression)\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Lasso, Ridge, ElasticNet"
   ]
  },
  {
   "source": [
    "### Import Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df2 = pd.read_csv('./data/kc_house_data.csv')"
   ]
  },
  {
   "source": [
    "#### Inspect Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr()"
   ]
  },
  {
   "source": [
    "#### Summarize Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe().transpose().round(2)"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df2.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data=df2.corr(), cmap='RdBu', vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr()['price'].sort_values(key=lambda x: np.abs(x), ascending=False).drop(['lat', 'long', 'price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.unique(df2['id']).size, df2.count()['id'])\n",
    "df2.drop(labels='id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.unique(df2['zipcode']).size, df2.count()['zipcode'])\n",
    "sns.relplot(data=df2, x='zipcode', y='price')\n",
    "df2.drop(labels='zipcode', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=4)\n",
    "d = np.array(ax).reshape((-1))\n",
    "for i, t in enumerate(['waterfront', 'floors', 'yr_renovated', 'sqft_lot', 'sqft_lot15', 'yr_built', 'condition']):\n",
    "    sns.scatterplot(data=df2, x=t, y='price', ax=d[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = df2.drop(labels=['waterfront', 'floors', 'yr_renovated', 'yr_built', 'condition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_B(df: pd.DataFrame):\n",
    "    top_features = df.corr()['price'].sort_values(key=lambda x: np.abs(x), ascending=False).drop(['lat', 'long', 'price'])[:9].index.values\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(8, 6))\n",
    "    axs = np.array(ax).reshape((-1))\n",
    "    for i, x in enumerate(top_features):\n",
    "        sns.scatterplot(data=df, x=x, y='price', ax=axs[i])\n",
    "    return fig\n",
    "f = plot_B(df2)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['date'] = pd.to_datetime(arg=df2['date'], yearfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df2, x='sqft_living', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(data=df2, x='grade', y='price')\n",
    "# sns.scatterplot(data=df2, x='grade', y='price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df2, x='bedrooms', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_tmp, x='bedrooms', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers\n",
    "outliers2 = df2[df2['']]"
   ]
  },
  {
   "source": [
    "#### Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "#### Feature Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_corr = df2.corr()\n",
    "df2_corr.drop(labels=df2_corr.columns[df2_corr.columns != 'price'].values, axis=1).drop(labels='price', axis=0)['price'].sort_values(ascending=False)"
   ]
  },
  {
   "source": [
    "### Data Partitioning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df2[['sqft_living', 'grade', 'sqft_above', 'sqft_living15', 'bathrooms', 'view', 'sqft_basement', 'bedrooms', 'waterfront']]\n",
    "y2 = df2['price']\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2)\n",
    "from sklearn.preprocessing import RobustScaler, normalize\n",
    "#! delete !#\n",
    "pip = Pipeline(steps=[\n",
    "    ('scaler', normalize),\n",
    "    ('linreg', LinearRegression())\n",
    "])\n",
    "model2 = LinearRegression()\n",
    "pip.fit(X=X2_train, y=y2_train)\n",
    "print(pip.score(X2_train, y2_train))\n",
    "print(pip.score(X2_test, y2_test))"
   ]
  },
  {
   "source": [
    "### Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LinearRegression()\n",
    "model2.fit(X=X2_train, y=y2_train)\n",
    "print(model2.score(X2_train, y2_train))\n",
    "print(model2.score(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = BayesianRidge()\n",
    "model2.fit(X=X2_train, y=y2_train)\n",
    "print(model2.score(X2_train, y2_train))\n",
    "print(model2.score(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "u = Pipeline(steps=[\n",
    "    ('scaler', QuantileTransformer()),\n",
    "    ('reg', Lasso())\n",
    "])\n",
    "\n",
    "params_ = [\n",
    "    {\n",
    "        'reg': [Lasso(), Ridge(), ElasticNet()],\n",
    "        'reg__alpha': np.linspace(0, 1, 5)\n",
    "    }\n",
    "]\n",
    "\n",
    "pd.DataFrame(GridSearchCV(u, params_).fit(X2_train, y2_train).cv_results_)"
   ]
  },
  {
   "source": [
    "### Model Scoring"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conclusions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}