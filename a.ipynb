{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd006e46f44bb844bdd55de1af887732b14e875f5b0d2374a3f4b9484cbcb737e08",
   "display_name": "Python 3.8.5  ('.venv': pipenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "06e46f44bb844bdd55de1af887732b14e875f5b0d2374a3f4b9484cbcb737e08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "source": [
    "# AIML CA1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import General Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Dependencies\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Graphing Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Dependencies\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Preservation Dependency\n",
    "import pickle\n",
    "\n",
    "# Miscellaneous Dependencies\n",
    "from typing import Callable, Dict, Union # static typing\n",
    "\n",
    "# Utility Functions\n",
    "from utils.extraction import extract_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide Warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "source": [
    "## Utility Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Part I"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Exclusive Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_mushrooms() -> pd.DataFrame:\n",
    "    # Extract raw content of ./data/agaricus-lepiota.names file\n",
    "    metadata: str\n",
    "    with open('./data/agaricus-lepiota.names') as f:\n",
    "        metadata = f.read()\n",
    "\n",
    "    # Extract attributes from metadata\n",
    "    attrs = extract_attributes(metadata, r'7\\. Attribute Information:.*\\n((.|\\n)*)8\\. Missing')\n",
    "\n",
    "    # Extract column names to be used for dataframe\n",
    "    cols = attrs.keys()\n",
    "\n",
    "    # Create the dataframe from ./data/agaricus-lepiota.data file,\n",
    "    #   using column names derived from ./data/agaricus-lepiota.names file\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer='./data/agaricus-lepiota.data',\n",
    "        sep=',',\n",
    "        header=0,\n",
    "        names=cols\n",
    "    )\n",
    "\n",
    "    # Expand attribute codes to their full definitions\n",
    "    for col in cols:\n",
    "        df[col].replace(to_replace=attrs[col] ,inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_mushrooms()"
   ]
  },
  {
   "source": [
    "#### Inspect Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect top 10 rows of the dataset\n",
    "df.head(n=10)"
   ]
  },
  {
   "source": [
    "#### Summarize Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect overview of the dataset\n",
    "df.info()"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to keep track of variables to be removed\n",
    "drop_cols = []"
   ]
  },
  {
   "source": [
    "Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isna().sum(axis=0)\n",
    "\n",
    "# Note that stalk-root has missing attributes (denoted as 'missing')\n",
    "# In fact, approx. 31% of the records have missing data for stalk-root\n",
    "stalk_dist = df['stalk-root'].value_counts()\n",
    "(stalk_dist / stalk_dist.sum()).round(2)\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols.append('stalk-root')"
   ]
  },
  {
   "source": [
    "Redundant Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unqiue counts of the individual features\n",
    "print(df.describe().transpose().sort_values(by='unique', ascending=False))\n",
    "\n",
    "# Note that veil-type has only one value,\n",
    "#   hence it is a redundant feature\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols.append('veil-type')"
   ]
  },
  {
   "source": [
    "Inspect the distribution of the target variable (class: edible/poisonous)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import format_label\n",
    "def plot_A(df: pd.DataFrame):\n",
    "    ax = sns.countplot(data=df, x='class', palette='deep')\n",
    "    ax.set_ylim(top=5000)\n",
    "    ax.set_title(label='General Data Distribution')\n",
    "    ax.set_ylabel(ylabel='Number of Records')\n",
    "    ax.set_yticklabels(labels=format_label(\n",
    "        ax.get_yticks() / 1000, lambda s: f'{round(s)}k'))\n",
    "    ax.set_xlabel(xlabel='Type')\n",
    "    total_count = df.shape[0]\n",
    "    for p in ax.patches:\n",
    "        x = p.get_x()\n",
    "        y = p.get_height()\n",
    "        ax.annotate(text=f'{y} ({y/total_count*100:.1f}%)',\n",
    "                    xy=(x + 0.21, y + 70))\n",
    "    return ax\n",
    "# plot_A(df=df)"
   ]
  },
  {
   "source": [
    "Inspect correlation between the independent variables and the target variable (class)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_B(df: pd.DataFrame):\n",
    "    for i in df.drop(labels='class', axis=1).columns.values:\n",
    "        fig, (corr_plot, freq_plot) = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "        ct = pd.crosstab(index=df['class'], columns=df[i])\n",
    "        distr = df.groupby(i).count().iloc[:,0]\n",
    "        proportion = (ct.iloc[1] - ct.iloc[0]) / distr\n",
    "        corr = pd.DataFrame(proportion.reset_index())\n",
    "        sns.barplot(data=corr, x=i, y=0, ax=corr_plot, color='grey')\n",
    "        sns.countplot(data=df.sort_values(by=i), x=i, hue='class', ax=freq_plot, palette='turbo')\n",
    "        fig.suptitle(t=f'{i.upper()}')\n",
    "        corr_plot.set_title(label='Correlation (chi2-based)')\n",
    "        corr_plot.set_ylim((-1.1, 1.1))\n",
    "        corr_plot.set_ylabel(ylabel='Correlation')\n",
    "        corr_plot.set_xticklabels(labels=corr_plot.get_xticklabels(), rotation=30)\n",
    "        freq_plot.set_title(label=f'Frequency Distribution')\n",
    "        freq_plot.set_xticklabels(labels=freq_plot.get_xticklabels(), rotation=30)\n",
    "# plot_B(df)"
   ]
  },
  {
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "There is no need for feature engineering in this dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Feature Selection\n",
    "\n",
    "There are 2 columns to be removed (stalk-root, veil-type)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns\n",
    "df.drop(labels=drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "### Encoding the data\n",
    "\n",
    "The data has only categorical text variables, therefore they<br>have to be converted to numeric form using dummy variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (One-Hot) Encode the dataset (categorical -> binary)\n",
    "df_ohe = pd.get_dummies(data=df, drop_first=True)"
   ]
  },
  {
   "source": [
    "### Inspect correlation after encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correlation between top 10 factors and target variable (class)\n",
    "df_ohe.corr()['class_poisonous'].drop(labels='class_poisonous').sort_values(key=lambda x: np.abs(x), ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi2-based feature selection\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Get top 10 factors that are correlated with the target variable (class)\n",
    "best_features_chi2 = SelectKBest(score_func=chi2, k=10).fit(X=df_ohe.drop(labels='class_poisonous', axis=1), y=df_ohe['class_poisonous'])\n",
    "best_features_mask = best_features_chi2.get_support()\n",
    "best_features = df_ohe.drop(labels='class_poisonous', axis=1).columns.values[best_features_mask]\n",
    "best_features_scores = best_features_chi2.scores_[best_features_mask]\n",
    "good_predictors = pd.Series(data=best_features_scores, index=best_features)\n",
    "\n",
    "good_predictors.sort_values(ascending=False)"
   ]
  },
  {
   "source": [
    "### Data Partitioning\n",
    "\n",
    "Split the data randomly into a train set and a test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X = df_ohe.drop(labels='class_poisonous', axis=1)\n",
    "y = df_ohe['class_poisonous']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "source": [
    "### Algorithm Selection & Hyper-Parameter Tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate classification algorithms\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "source": [
    "#### Determine best candidate algorithm using GridSearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_clf():\n",
    "    cand_pipe_1 = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', DummyEstimator())\n",
    "    ])\n",
    "\n",
    "    cand_params_1 = [\n",
    "        {\n",
    "            'clf': [KNeighborsClassifier()],\n",
    "            'clf__n_neighbors': np.arange(3, 14, 2)\n",
    "        },\n",
    "        {\n",
    "            'clf': [LogisticRegression()],\n",
    "            'clf__solver': ['liblinear', 'newton-cg'],\n",
    "            'clf__C': np.logspace(-3, 3, 3),\n",
    "            'clf__multi_class': ['ovr']\n",
    "        },\n",
    "        {\n",
    "            'clf': [CategoricalNB()],\n",
    "            'clf__alpha': np.logspace(-3, 3, 6)\n",
    "        },\n",
    "        {\n",
    "            'clf': [SVC()],\n",
    "            'clf__kernel': ['rbf', 'poly'],\n",
    "            'clf__C': np.logspace(-3, 4, 3)\n",
    "        },\n",
    "        {\n",
    "            'clf': [DecisionTreeClassifier()],\n",
    "            'clf__max_depth': [10, 20, 30],\n",
    "            'clf__min_samples_leaf': [10, 30]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    best_clf_algo = GridSearchCV(estimator=cand_pipe_1, param_grid=cand_params_1, cv=3)\n",
    "    best_clf_algo.fit(X=X, y=y)\n",
    "    return best_clf_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=grid_search_clf(), file=open('./models/best_clf_algo.p', 'wb'))\n",
    "\n",
    "# Load result\n",
    "best_clf_algo_loaded = pickle.load(file=open('./models/best_clf_algo.p', 'rb'))\n",
    "\n",
    "# Inspect result\n",
    "print(best_clf_algo_loaded.best_estimator_)\n",
    "gs_clf = pd.DataFrame(best_clf_algo_loaded.cv_results_)\n",
    "gs_clf.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "#### Determine best hyperparameters for selected algorithm using GridSearch\n",
    "\n",
    "Selected algorithm: logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_clf_params():\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('scaler', DummyScaler()),\n",
    "        ('clf', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    params = {\n",
    "        'scaler': ['passthrough', StandardScaler()],\n",
    "        'clf__solver': ['liblinear', 'saga'],\n",
    "        'clf__tol': np.logspace(-5, 2, 3),\n",
    "        'clf__C': np.logspace(-4, 4, 5),\n",
    "        'clf__multi_class': ['ovr']\n",
    "    }\n",
    "\n",
    "    best_clf_params = GridSearchCV(estimator=pipe, param_grid=params, cv=5, n_jobs=-1)\n",
    "    best_clf_params.fit(X=X, y=y)\n",
    "    return best_clf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=grid_search_clf_params(), file=open('./models/best_clf_params.p', 'wb'))\n",
    "\n",
    "# Load result\n",
    "best_clf_params = pickle.load(file=open('./models/best_clf_params.p', 'rb'))\n",
    "\n",
    "# Inspect result\n",
    "print(best_clf_params.best_params_)\n",
    "gs_clf_params = pd.DataFrame(best_clf_params.cv_results_)\n",
    "gs_clf_params.sort_values(by='rank_test_score')"
   ]
  },
  {
   "source": [
    "### Check for Overfitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = df_results.copy()\n",
    "t_d['param_clf'].astype('str')\n",
    "# ddd = t_d.groupby('param_clf').mean()\n",
    "# try_out = ddd.reset_index().melt(id_vars='param_clf', var_name='test', value_name='s')\n",
    "# try_out['test'] = try_out['test'].str.slice(5, 6).astype(int)\n",
    "# try_out['test'] = try_out['test'].str.extract(pat=r'*([\\d])*', expand=False)\n",
    "# sns.lineplot(data=try_out, x='test', y='s', hue='param_clf')\n",
    "# try_out\n",
    "\n",
    "fig, ax = plt.subplots(ncols=5, sharey=True, figsize=(12, 8))\n",
    "hyp = ['param_clf__n_neighbors', 'param_clf__C', None, 'param_clf__C', 'param_clf__max_depth']\n",
    "for i, est in enumerate(pd.unique(t_d['param_clf'])):\n",
    "    stuff = t_d[t_d['param_clf'] == est].melt(id_vars=['param_clf', 'param_clf__n_neighbors', 'param_clf__C', 'param_clf__max_depth'], value_vars=['split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score'], var_name='test', value_name='score')\n",
    "    stuff['test'] = stuff['test'].str.extract(pat='([\\\\d])', expand=False)\n",
    "    stuff['test'] = stuff['test'].astype(int)\n",
    "    stuff['test'] += 1\n",
    "    stuff.dropna(axis=1, inplace=True)\n",
    "    ax[i].set_ylim((0.5, 1.2))\n",
    "    ax[i].set_title(est)\n",
    "    ax[i].set_xticks(ticks=range(1, 6))\n",
    "    sns.lineplot(data=stuff, x='test', y='score', hue=hyp[i], ax=ax[i], palette='muted')"
   ]
  },
  {
   "source": [
    "### Building Pipeline\n",
    "<br>\n",
    "Build a machine learning pipeline, using\n",
    "\n",
    "*   a one-hot encoder,\n",
    "*   a custom feature-selection transformer,\n",
    "*   a standard scaler,\n",
    "*   the most consistent algorithm,\n",
    "*   the best performing hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return pd.get_dummies(data=X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_cols_1(df: pd.DataFrame):\n",
    "    return df.drop(labels=drop_cols, axis=1)\n",
    "\n",
    "class FeatureSelector1(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(drop_redundant_cols_1(X).columns.values)\n",
    "        return drop_redundant_cols_1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import encoder\n",
    "# from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[\n",
    "    ('transform', FeatureSelector1()),\n",
    "    ('encode', OHEncoder()),\n",
    "    ('classifier', LogisticRegression(C=100.0, multi_class='ovr', solver='liblinear', tol=1e-05))\n",
    "])"
   ]
  },
  {
   "source": [
    "### Redefine Data Partition\n",
    "\n",
    "With the relevant transformers in place, data pre-processing<br>has been integrated into the machine learning pipeline\n",
    "\n",
    "Therefore, the data should be retrieved from the original source and re-partitioned"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_mushrooms()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(labels='class', axis=1), df['class'], random_state=2)"
   ]
  },
  {
   "source": [
    "### Model Training\n",
    "\n",
    "Fit the data to the pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X=X_train, y=y_train)\n",
    "\n",
    "# pickle.dump(obj=model, file=open('./models/final_classifier.p', 'wb'))\n",
    "\n",
    "# final_classifier = pickle.load(file=open('./models/final_classifier.p', 'rb'))"
   ]
  },
  {
   "source": [
    "### Model Scoring\n",
    "\n",
    "Use the model to generate predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X=X_test)\n",
    "y_pred"
   ]
  },
  {
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluate the performance of the final model based on standard classification metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model evaluation dependencies\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "source": [
    "#### Evaluate against train set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X=X_train)\n",
    "\n",
    "# Classification summary\n",
    "print(classification_report(y_true=y_train, y_pred=y_train_pred, target_names=['edible', 'poisonous']))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\n', pd.DataFrame(data=confusion_matrix(y_true=y_train, y_pred=y_train_pred, labels=['edible', 'poisonous']), index=['Actual Edible', 'Actual Poisonous'], columns=['Predicted Edible', 'Predicted Poisonous']), '\\n\\n', sep='')\n",
    "\n",
    "# Print target distribution in y_test\n",
    "print(y_train.groupby(y_train).count())"
   ]
  },
  {
   "source": [
    "#### Evaluate against test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification summary\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred, target_names=['edible', 'poisonous']))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\n', pd.DataFrame(data=confusion_matrix(y_true=y_test, y_pred=y_pred, labels=['edible', 'poisonous']), index=['Actual Edible', 'Actual Poisonous'], columns=['Predicted Edible', 'Predicted Poisonous']), '\\n\\n', sep='')\n",
    "\n",
    "# Print target distribution in y_test\n",
    "print(y_test.groupby(y_test).count())"
   ]
  },
  {
   "source": [
    "## Part II"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Data\n",
    "\n",
    "Load data about King County house sales"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from a csv file\n",
    "df2 = pd.read_csv('./data/kc_house_data.csv')"
   ]
  },
  {
   "source": [
    "#### Inspect Data\n",
    "\n",
    "Preview a sample of the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the top 10 rows of the dataset\n",
    "df2.head(n=10)"
   ]
  },
  {
   "source": [
    "#### Summarize Data\n",
    "\n",
    "Get a sense of the features involved"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect overview of the dataset\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect statistics of the dataset\n",
    "df2.describe().transpose().round(2)"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Exploratory Data Analysis (EDA)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to keep track of variables to be removed\n",
    "drop_cols_2 = []\n",
    "\n",
    "# List to keep track of positively skewed variables\n",
    "positively_skewed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df2.isna().sum(axis=0)\n",
    "\n",
    "# There doesn't seem to be any missing values"
   ]
  },
  {
   "source": [
    "Visualize correlation amongst the original features using a heatmap"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2A():\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(data=df2.corr(), cmap='RdBu', vmin=-1, vmax=1, ax=ax)\n",
    "# plot_2A()"
   ]
  },
  {
   "source": [
    "Inspect distribution of the individual variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2B():\n",
    "    for i in df2.columns.values:\n",
    "        if df2[i].dtype.kind in 'biufc':\n",
    "            fig, (hst, bxp) = plt.subplots(ncols=2)\n",
    "            sns.histplot(data=df2, x=i, ax=hst)\n",
    "            sns.boxplot(data=df2, y=i, ax=bxp)\n",
    "# plot_2B()\n",
    "\n",
    "# Many of the features seem to be positively skewed\n",
    "\n",
    "# Course of action - logarithmic transformation\n",
    "transform_cols_2.extend(['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'long', 'sqft_living15', 'sqft_lot15'])"
   ]
  },
  {
   "source": [
    "Inspect correlation between the features and the target variable (price)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_E(df: pd.DataFrame):\n",
    "    for i in positively_skewed:\n",
    "        perc = df[df[i] == 0].shape[0] / df.shape[0] * 100\n",
    "        fig, (bef, aft) = plt.subplots(ncols=2)\n",
    "        bef.set_title(label='Before Logarithmic Transformation')\n",
    "        aft.set_title(label='After Logarithmic Transformation')\n",
    "        sns.histplot(data=df, x=i, ax=bef)\n",
    "        sns.histplot(data=np.log1p(df[i]), ax=aft)\n",
    "# plot_E(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr()['price'].sort_values(key=lambda x: np.abs(x), ascending=False)"
   ]
  },
  {
   "source": [
    "Inspect absolute correlation between features and target variable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2D():\n",
    "    fe = FeatureEngineering()\n",
    "    fs = FeatureSelection()\n",
    "    sns.heatmap(fe.fit_transform(pd.read_csv('./data/kc_house_data.csv')).corr().abs(), vmin=0, vmax=1, cmap='Blues')\n",
    "# plot_2D()"
   ]
  },
  {
   "source": [
    "Inspect id feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check id data type\n",
    "print(f'ID data type: {df2['id'].dtype}')\n",
    "\n",
    "# Compare the number of ids to the total number of records \n",
    "print(f'Number of unique IDs: {pd.unique(df2['id']).size}')\n",
    "print(f'Total number of records: {df2.shape[0]}')\n",
    "\n",
    "# Check correlation between id and the rest of the variables\n",
    "print(df2.corr()['id'])\n",
    "\n",
    "\n",
    "# id seems redundant\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols_2.append('id')"
   ]
  },
  {
   "source": [
    "Inspect zipcode feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check zipcode data type\r\n",
    "print(f'zipcode data type: {df2['zipcode'].dtype}')\r\n",
    "\r\n",
    "# Compare the number of zipcodes to the total number of records \r\n",
    "print(f'Number of unique zipcodes: {pd.unique(df2['zipcode']).size}')\r\n",
    "print(f'Total number of records: {df2.shape[0]}')\r\n",
    "\r\n",
    "# Check correlation between zipcode and the rest of the variables\r\n",
    "print(df2.corr()['zipcode'])\r\n",
    "\r\n",
    "\r\n",
    "# zipcode does not seem redundant\r\n",
    "\r\n",
    "# Course of action - no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_F():\n",
    "    for i in positively_skewed:\n",
    "        fig, (bef, aft, aft_ag) = plt.subplots(ncols=3)\n",
    "        sns.scatterplot(data=df2, x=i, y='price', ax=bef)\n",
    "        sns.scatterplot(x=np.sqrt(df2[i]), y=np.sqrt(df2['price']), ax=aft)\n",
    "        sns.scatterplot(x=(df2[i]), y=np.sqrt(df2['price']), ax=aft_ag)"
   ]
  },
  {
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "There seems to be useful extractable data in the `date` feature"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year, month and day from the date feature\n",
    "df2_date = pd.to_datetime(df2['date'], yearfirst=True)\n",
    "df2['year'] = pd.DatetimeIndex(data=df2_date).year\n",
    "df2['month'] = pd.DatetimeIndex(data=df2_date).month\n",
    "df2['day'] = pd.DatetimeIndex(data=df2_date).day\n",
    "\n",
    "# Date variable seems redundant now\n",
    "\n",
    "# Course of action - drop column\n",
    "drop_cols_2.append('date')"
   ]
  },
  {
   "source": [
    "#### Feature Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review columns to be dropped\n",
    "drop_cols_2"
   ]
  },
  {
   "source": [
    "There are 2 columns to be removed (`id`, `date`)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns\n",
    "df2.drop(labels=drop_cols_2, axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "### Data Partitioning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_another = df2.copy()\n",
    "sns.heatmap(data=df2[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'sqft_lot15']].corr(), cmap='RdBu', vmin=-1, vmax=1, annot=True)\n",
    "positively_skewed = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "dfp = df_another['price']\n",
    "df_another = np.log1p(df_another[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'sqft_lot15']])\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(data=df_another.corr(), cmap='RdBu', vmin=-1, vmax=1, ax=ax, annot=True)"
   ]
  },
  {
   "source": [
    "### Algorithm Selection & Hyper-Parameter Tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate regression algorithms\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "source": [
    "Determine best regression algorithm using GridSearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer, FunctionTransformer, RobustScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "class DummyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self): pass\n",
    "    def fit_transform(self): pass\n",
    "    def transform(self): pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# expt - use df_another\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(df2.drop('price', axis=1), df2['price'], test_size=0.2)\n",
    "\n",
    "u = Pipeline(steps=[\n",
    "    # ('trans', CustomTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reg', DummyEstimator())\n",
    "])\n",
    "\n",
    "\n",
    "params_ = [\n",
    "    {\n",
    "        'reg': [LinearRegression()],\n",
    "        'reg__normalize': [True, False],\n",
    "        'reg__fit_intercept': [True, False]\n",
    "    },\n",
    "    {\n",
    "        'reg': [Lasso(), Ridge()],\n",
    "        'reg__alpha': np.logspace(-5, 3, 6)\n",
    "    },\n",
    "    {\n",
    "        'reg': [DecisionTreeRegressor()],\n",
    "        'reg__max_depth': np.arange(5, 11)\n",
    "    },\n",
    "    {\n",
    "        'reg': [KNeighborsRegressor()],\n",
    "        'reg__n_neighbors': np.arange(5, 11)\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "lol = GridSearchCV(u, params_, n_jobs=3)\n",
    "lol.fit(X3_train, y3_train)\n",
    "\n",
    "results = pd.DataFrame(lol.cv_results_).sort_values(by='rank_test_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=best_reg_algo, file=open('./models/best_reg_algo.p', 'wb'))\n",
    "\n",
    "# Inspect result\n",
    "best_reg_algo_loaded = pickle.load(file=open('./models/best_reg_algo.p', 'rb'))\n",
    "\n",
    "print(best_reg_algo_loaded.best_estimator_)\n",
    "best_reg_algo_loaded.cv_results_"
   ]
  },
  {
   "source": [
    "Determine best hyperparameters for selected algorithm using GridSearch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        all_cols = X.columns.values\n",
    "        for i in all_cols:\n",
    "            if i == 'long':\n",
    "                X_copy.drop(labels='long', axis=1, inplace=True)\n",
    "            elif i in positively_skewed:\n",
    "                X_copy[i] = np.log1p(X[i])\n",
    "        return X_copy\n",
    "\n",
    "dtreepipe = Pipeline(steps=[\n",
    "    # ('transformer', CustomTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "dtreeparams = {\n",
    "    'regressor__max_depth': np.arange(2, 25, 2),\n",
    "    'regressor__min_samples_split': np.arange(10, 31, 10),\n",
    "    'regressor__min_samples_leaf': np.arange(10, 300, 50)\n",
    "}\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "dtreeCV = RandomizedSearchCV(estimator=dtreepipe, param_distributions=dtreeparams, cv=3, n_jobs=-1)\n",
    "\n",
    "dtreeCV.fit(X3_train, y3_train)\n",
    "dtreeCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "# pickle.dump(obj=best_reg_hyper_params, file=open('./models/best_reg_hyper_params.p', 'wb'))\n",
    "\n",
    "# Inspect result\n",
    "best_reg_hyper_params_loaded = pickle.load(file=open('./models/best_reg_algo.p', 'rb'))\n",
    "\n",
    "print(best_reg_hyper_params_loaded.best_estimator_)\n",
    "best_reg_algo_loaded.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=['rank_test_score', 'std_test_score']).iloc[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pickle.load(file=open('./models/grid_search_reg.p', 'rb'))\n",
    "# pickle.dump(obj=results, file=open('./models/grid_search_reg.p', 'wb'))\n"
   ]
  },
  {
   "source": [
    "Further tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEST CASE\n",
    "# ogp = Pipeline(steps=[\n",
    "#     ('scale', StandardScaler()),\n",
    "#     ('reg', BaggingRegressor(GradientBoostingRegressor(max_depth=20, min_samples_leaf=10, min_samples_split=30), n_jobs=-1)\n",
    "# )])\n",
    "\n",
    "ogp = Pipeline(steps=[\n",
    "    ('feature_engineer', FeatureEngineer2()),\n",
    "    ('feature_selector', FeatureSelector2()),\n",
    "    ('scale', CustomTransformer()),\n",
    "    ('reg', BaggingRegressor(GradientBoostingRegressor(max_depth=20, min_samples_leaf=10, min_samples_split=30), n_jobs=-1)\n",
    ")])\n",
    "\n",
    "bestboi = TransformedTargetRegressor(regressor=ogp, func=np.sqrt, inverse_func=np.square, check_inverse=False)\n",
    "\n",
    "bestboi.fit(X3_train, y3_train)\n",
    "\n",
    "bestboi.score(X3_test, y3_test)\n",
    "\n",
    "# pickle.dump(obj=bestboi, file=open('./models/best_reg_model.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor\n",
    "\n",
    "class CustomTransformer2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        # X_copy[]\n",
    "        return X_copy\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(df2.drop('price', axis=1), df2['price'], test_size=0.25)\n",
    "\n",
    "latest_dtree = Pipeline(steps=[\n",
    "    # ('cust', CustomTransformer2()),\n",
    "    ('scale', StandardScaler()),\n",
    "    # ('check', CustomTransformer()),\n",
    "    ('reg', GradientBoostingRegressor(max_depth=20, min_samples_leaf=10, min_samples_split=30)\n",
    ")])\n",
    "\n",
    "noscale = Pipeline(steps=[\n",
    "    # ('cust', CustomTransformer()),\n",
    "    # ('scale', StandardScaler()),\n",
    "    ('reg', GradientBoostingRegressor(max_depth=20, min_samples_leaf=10, min_samples_split=30)\n",
    ")])\n",
    "\n",
    "log_price = TransformedTargetRegressor(regressor=latest_dtree, func=np.sqrt, inverse_func=np.square, check_inverse=False)\n",
    "\n",
    "rt_nos = TransformedTargetRegressor(regressor=noscale, func=np.sqrt, inverse_func=np.square, check_inverse=False)\n",
    "\n",
    "mos = ['nope', 'trans', 'noscale', 'noscaler trans']\n",
    "for j, mo in enumerate([latest_dtree, log_price, noscale, rt_nos]):\n",
    "    mo.fit(X3_train, y3_train)\n",
    "    tr_sc = r2_score(y3_train, mo.predict(X3_train))\n",
    "    ts_sc = r2_score(y3_test, mo.predict(X3_test))\n",
    "    print(mos[j], tr_sc - ts_sc, tr_sc, ts_sc)"
   ]
  },
  {
   "source": [
    "### Combining everything"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Building the pipeline\n",
    "<br>\n",
    "Build the machine learning pipeline, using\n",
    "\n",
    "*   a custom feature-engineering transformer,\n",
    "*   a custom feature-selection transformer,\n",
    "*   a custom logarithmic transformer,\n",
    "*   a standard scaler,\n",
    "*   the most consistent algorithm (gradient boosting regressor),\n",
    "*   the best performing hyperparameters\n",
    "\n",
    "To further improve performance and reduce overfitting,<br>\n",
    "the target variable will be transformed too (sqrt)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_parts(df: pd.DataFrame, col: str, **kwargs):\n",
    "    df_datetime = pd.DatetimeIndex(df[col], **kwargs)\n",
    "    return df_datetime.year, df_datetime.month, df_datetime.day\n",
    "\n",
    "class FeatureEngineer2(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_year, X_month, X_day = extract_date_parts(df=X, col='date', yearfirst=True)\n",
    "        X_copy['year'] = X_year\n",
    "        X_copy['month'] = X_month\n",
    "        X_copy['day'] = X_day\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redundant_features(df: pd.DataFrame):\n",
    "    return df.drop(labels=['id', 'zipcode', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "def exclude_target_variable(df: pd.DataFrame):\n",
    "    return df.drop(labels='price', axis=1)\n",
    "\n",
    "class FeatureSelector2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        has_target_variable: bool = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if 'price' in X.columns.values:\n",
    "            self.has_target_variable = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = drop_redundant_features(X)\n",
    "        if self.has_target_variable:\n",
    "            X_copy = exclude_target_variable(X_copy)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "pipe2 = Pipeline(steps=[\n",
    "    ('feature_engineer', FeatureEngineer2()),\n",
    "    ('feature_selector', FeatureSelector2()),\n",
    "    ('log_transformer', LogTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', GradientBoostingRegressor(max_depth=20, min_samples_leaf=10, min_samples_split=30))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap pipeline in a target transformer\n",
    "model2 = TransformedTargetRegressor(regressor=pipe2, func=np.sqrt, inverse_func=np.square, check_inverse=False)"
   ]
  },
  {
   "source": [
    "### Model Training\n",
    "\n",
    "Fit the data to the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(X=X2_train, y=y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "pickle.dump(obj=model2, file=open('./models/final_regressor.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "final_regressor = pickle.load(file=open('./models/final_regressor.p', 'rb'))"
   ]
  },
  {
   "source": [
    "### Model Scoring\n",
    "\n",
    "Use the model to generate predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = final_regressor.predict(X2_test)\n",
    "y_pred_2"
   ]
  },
  {
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluate the performance of the final model based on standard regression metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regression metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_report(y_true, y_pred, type: str):\n",
    "    print(\n",
    "f'''Regression Report ({type})\n",
    "================================\n",
    "MSE:\\t\\t{np.round(mean_squared_error(y_true=y_true, y_pred=y_pred), 2)}\n",
    "MAE:\\t\\t{np.round(mean_absolute_error(y_true=y_true, y_pred=y_pred), 2)}\n",
    "R2:\\t\\t{np.round(r2_score(y_true=y_true, y_pred=y_pred), 4)}\n",
    "''')"
   ]
  },
  {
   "source": [
    "#### Evaluate against training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report(y2_train, final_regressor.predict(X2_train))"
   ]
  },
  {
   "source": [
    "#### Evaluate against testing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_report(y2_test, y_pred_2)"
   ]
  },
  {
   "source": [
    "#### Evaluate against entire dataset (visualization)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('./data/kc_house_data.csv')\n",
    "\n",
    "df_new.drop(labels=['id', 'zipcode'], axis=1, inplace=True)\n",
    "df_date = pd.to_datetime(df_new['date'], yearfirst=True)\n",
    "df_new['year'] = pd.DatetimeIndex(data=df_date).year\n",
    "df_new['month'] = pd.DatetimeIndex(data=df_date).month\n",
    "df_new['day'] = pd.DatetimeIndex(data=df_date).day\n",
    "df_new.drop(labels='date', axis=1, inplace=True)\n",
    "\n",
    "for noth in range(20):\n",
    "    df_new = df_new.sample(frac=1)\n",
    "    fig, ax = plt.subplots()\n",
    "    scores = []\n",
    "    buffs = []\n",
    "    for buf in np.arange(2, 50, 2):\n",
    "        buffs.append(buf)\n",
    "        scores.append(mb.score(df_new.drop('price', axis=1).iloc[:buf,:], df_new['price'].iloc[:buf]))\n",
    "    sns.lineplot(x=buffs, y=scores, ax=ax)\n",
    "    ax.set_ylim(0, 1)\n",
    "    sns.lineplot(x=[0, 50], y=[0.9] * 2, color='orange', ax=ax)\n",
    "    sns.lineplot(x=[0, 50], y=[0.8] * 2, color='red', ax=ax)\n",
    "    sns.lineplot(x=[10] * 2, y=[0, 1.], color='grey', ax=ax)"
   ]
  },
  {
   "source": [
    "## Conclusions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}