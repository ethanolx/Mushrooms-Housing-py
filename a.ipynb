{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd006e46f44bb844bdd55de1af887732b14e875f5b0d2374a3f4b9484cbcb737e08",
   "display_name": "Python 3.8.5  ('.venv': pipenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "06e46f44bb844bdd55de1af887732b14e875f5b0d2374a3f4b9484cbcb737e08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "source": [
    "# AIML CA1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import General Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Dependencies\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "# Graphing Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Dependencies\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Miscellaneous Dependencies\n",
    "from typing import Callable, Dict # static typing\n",
    "\n",
    "# Utility Functions\n",
    "from utils.extraction import extract_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide Warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "source": [
    "## Utility Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Part I"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Exclusive Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Classification Metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "source": [
    "### Import Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_mushrooms() -> pd.DataFrame:\n",
    "    # Extract raw content of ./data/agaricus-lepiota.names file\n",
    "    metadata: str\n",
    "    with open('./data/agaricus-lepiota.names') as f:\n",
    "        metadata = f.read()\n",
    "\n",
    "    # Extract attributes from metadata\n",
    "    attrs = extract_attributes(metadata, r'7\\. Attribute Information:.*\\n((.|\\n)*)8\\. Missing')\n",
    "\n",
    "    # Extract column names to be used for dataframe\n",
    "    cols = attrs.keys()\n",
    "\n",
    "    # Create the dataframe from ./data/agaricus-lepiota.data file,\n",
    "    #   using column names derived from ./data/agaricus-lepiota.names file\n",
    "    df = pd.read_csv(\n",
    "        filepath_or_buffer='./data/agaricus-lepiota.data',\n",
    "        sep=',',\n",
    "        header=0,\n",
    "        names=cols\n",
    "    )\n",
    "\n",
    "    # Expand attribute codes to their full definitions\n",
    "    for col in cols:\n",
    "        df[col].replace(to_replace=attrs[col] ,inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_mushrooms()"
   ]
  },
  {
   "source": [
    "#### Inspect Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect top 10 rows of the dataset\n",
    "df.head(n=10)"
   ]
  },
  {
   "source": [
    "#### Summarize Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect overview of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect statistics of the dataset\n",
    "df.describe().transpose().sort_values(by='unique', ascending=False)\n",
    "\n",
    "# Note that veil-type has only one value,\n",
    "#   hence it is redundant\n",
    "df.drop(labels='veil-type', axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Target Distribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import format_label\n",
    "def plot_A(df: pd.DataFrame):\n",
    "    ax = sns.countplot(data=df, x='class', palette='deep')\n",
    "    ax.set_ylim(top=5000)\n",
    "    ax.set_title(label='General Data Distribution')\n",
    "    ax.set_ylabel(ylabel='Number of Records')\n",
    "    ax.set_yticklabels(labels=format_label(\n",
    "        ax.get_yticks() / 1000, lambda s: f'{round(s)}k'))\n",
    "    ax.set_xlabel(xlabel='Type')\n",
    "    total_count = df.shape[0]\n",
    "    for p in ax.patches:\n",
    "        x = p.get_x()\n",
    "        y = p.get_height()\n",
    "        ax.annotate(text=f'{y} ({y/total_count*100:.1f}%)',\n",
    "                    xy=(x + 0.21, y + 70))\n",
    "    return ax\n",
    "ax_a = plot_A(df=df)\n",
    "ax_a"
   ]
  },
  {
   "source": [
    "Missing Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isna().sum(axis=0)\n",
    "\n",
    "# Note that stalk-root has missing attributes (denoted as 'missing')\n",
    "df['stalk-root'] = df['stalk-root'].str.replace(pat='missing', repl='unknown')"
   ]
  },
  {
   "source": [
    "Correlation Between Attributes and Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_B(df: pd.DataFrame):\n",
    "    for i in df.drop(labels='class', axis=1).columns.values:\n",
    "        fig, (corr_plot, freq_plot) = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "        ct = pd.crosstab(index=df['class'], columns=df[i])\n",
    "        distr = df.groupby(i).count().iloc[:,0]\n",
    "        proportion = (ct.iloc[1] - ct.iloc[0]) / distr\n",
    "        corr = pd.DataFrame(proportion.reset_index())\n",
    "        sns.barplot(data=corr, x=i, y=0, ax=corr_plot, color='grey')\n",
    "        sns.countplot(data=df.sort_values(by=i), x=i, hue='class', ax=freq_plot, palette='turbo')\n",
    "        fig.suptitle(t=f'{i.upper()}')\n",
    "        corr_plot.set_title(label='Correlation (chi2-based)')\n",
    "        corr_plot.set_ylim((-1.1, 1.1))\n",
    "        corr_plot.set_ylabel(ylabel='Correlation')\n",
    "        corr_plot.set_xticklabels(labels=corr_plot.get_xticklabels(), rotation=30)\n",
    "        freq_plot.set_title(label=f'Frequency Distribution')\n",
    "        freq_plot.set_xticklabels(labels=freq_plot.get_xticklabels(), rotation=30)\n",
    "plot_B(df)"
   ]
  },
  {
   "source": [
    "#### Feature Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (One-Hot) Encode the dataset (categorical -> binary)\n",
    "df_ohe = pd.get_dummies(data=df, drop_first=True)\n",
    "\n",
    "# Get correlation between top 10 factors and Class\n",
    "df_ohe.corr()['class_poisonous'].drop(labels='class_poisonous').sort_values(key=lambda x: np.abs(x), ascending=False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi2-based feature selection\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Get top 10 factors that are correlated with Class\n",
    "mask = SelectKBest(score_func=chi2, k=10).fit(X=df_ohe.drop(labels='class_poisonous', axis=1), y=df_ohe[['class_poisonous']])\n",
    "good_preds = df_ohe.drop(labels='class_poisonous', axis=1).columns.values[mask.get_support()]\n",
    "good_preds"
   ]
  },
  {
   "source": [
    "### Data Partitioning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X = df_ohe.drop(labels='class_poisonous', axis=1)\n",
    "y = df_ohe['class_poisonous']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "source": [
    "### Algorithm Selection & Hyper-Parameter Tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import candidate classification algorithms\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('scaler', StandardScaler()),\n",
    "#     ('clf', DummyEstimator())\n",
    "# ])\n",
    "\n",
    "# params = [\n",
    "#     {\n",
    "#         'clf': [KNeighborsClassifier()],\n",
    "#         'clf__n_neighbors': np.arange(start=4, stop=10)\n",
    "#     },\n",
    "#     {\n",
    "#         'clf': [LogisticRegression(solver='newton-cg')],\n",
    "#         'clf__C': np.logspace(-1, 2, 3)\n",
    "#     },\n",
    "#     {\n",
    "#         'clf': [MultinomialNB()]\n",
    "#     },\n",
    "#     {\n",
    "#         'clf': [SVC()],\n",
    "#         'clf__C': np.logspace(-1, 2, 3)\n",
    "#     },\n",
    "#     {\n",
    "#         'clf': [DecisionTreeClassifier()],\n",
    "#         'clf__max_depth': [10, 20, 30]\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# cv = GridSearchCV(estimator=pipeline, param_grid=params, cv=5)\n",
    "# cv.fit(X=X, y=y)\n",
    "# import pickle\n",
    "# pickle.dump(obj=cv, file=open(\"./models/grid_search_clf.p\", \"wb\"))\n",
    "# print(cv.best_params_)\n",
    "# print(cv.best_score_)\n",
    "# print(cv.best_estimator_)\n",
    "# pd.DataFrame(data=cv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "mod = pickle.load(file=open('./models/grid_search_clf.p', 'rb'))\n",
    "df_results = pd.DataFrame(mod.cv_results_)\n",
    "# df_results.to_csv('./tmp/a.csv')\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = df_results.copy()\n",
    "t_d['param_clf'].astype('str')\n",
    "# ddd = t_d.groupby('param_clf').mean()\n",
    "# try_out = ddd.reset_index().melt(id_vars='param_clf', var_name='test', value_name='s')\n",
    "# try_out['test'] = try_out['test'].str.slice(5, 6).astype(int)\n",
    "# try_out['test'] = try_out['test'].str.extract(pat=r'*([\\d])*', expand=False)\n",
    "# sns.lineplot(data=try_out, x='test', y='s', hue='param_clf')\n",
    "# try_out\n",
    "\n",
    "fig, ax = plt.subplots(ncols=5, sharey=True, figsize=(12, 8))\n",
    "hyp = ['param_clf__n_neighbors', 'param_clf__C', None, 'param_clf__C', 'param_clf__max_depth']\n",
    "for i, est in enumerate(pd.unique(t_d['param_clf'])):\n",
    "    stuff = t_d[t_d['param_clf'] == est].melt(id_vars=['param_clf', 'param_clf__n_neighbors', 'param_clf__C', 'param_clf__max_depth'], value_vars=['split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score'], var_name='test', value_name='score')\n",
    "    stuff['test'] = stuff['test'].str.extract(pat='([\\\\d])', expand=False)\n",
    "    stuff['test'] = stuff['test'].astype(int)\n",
    "    stuff['test'] += 1\n",
    "    stuff.dropna(axis=1, inplace=True)\n",
    "    ax[i].set_ylim((0.5, 1.2))\n",
    "    ax[i].set_title(est)\n",
    "    ax[i].set_xticks(ticks=range(1, 6))\n",
    "    sns.lineplot(data=stuff, x='test', y='score', hue=hyp[i], ax=ax[i], palette='muted')"
   ]
  },
  {
   "source": [
    "### Model Training\n",
    "\n",
    "Fit the data to the most consistent algorithm, using the best performing hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=3.16)\n",
    "clf.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "source": [
    "### Model Scoring\n",
    "\n",
    "Use the model to generate predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X=X_test)\n",
    "y_pred"
   ]
  },
  {
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Evaluate the performance of the model based on common metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model evaluation dependencies\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification summary\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred, target_names=['edible', 'poisonous']))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\n', pd.DataFrame(data=confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[0, 1]), index=['Actual Edible', 'Actual Poisonous'], columns=['Predicted Edible', 'Predicted Poisonous']), '\\n\\n', sep='')\n",
    "\n",
    "# Print target distribution in y_test\n",
    "print(y_test.groupby(y_test).count())"
   ]
  },
  {
   "source": [
    "## Part II"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import Exclusive Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Models (Regression)\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, Lasso, Ridge, ElasticNet"
   ]
  },
  {
   "source": [
    "### Import Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df2 = pd.read_csv('./data/kc_house_data.csv')"
   ]
  },
  {
   "source": [
    "#### Inspect Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr()"
   ]
  },
  {
   "source": [
    "#### Summarize Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe().transpose().round(2)"
   ]
  },
  {
   "source": [
    "### Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df2.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_date = pd.to_datetime(df2['date'], yearfirst=True)\n",
    "df2['year'] = pd.DatetimeIndex(data=df2_date).year\n",
    "df2['month'] = pd.DatetimeIndex(data=df2_date).month\n",
    "df2['day'] = pd.DatetimeIndex(data=df2_date).day\n",
    "df2.drop(labels='date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_C():\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(data=df2.corr(), cmap='RdBu', vmin=-1, vmax=1, ax=ax)\n",
    "plot_C()"
   ]
  },
  {
   "source": [
    "Distribution of Individual Variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_D():\n",
    "    for i in df2.columns.values:\n",
    "        if df2[i].dtype.kind in 'biufc':\n",
    "            fig, (hst, bxp) = plt.subplots(ncols=2)\n",
    "            sns.histplot(data=df2, x=i, ax=hst)\n",
    "            sns.boxplot(data=df2, y=i, ax=bxp)\n",
    "plot_D()\n",
    "positively_skewed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positively_skewed = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'long', 'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "def plot_E(df: pd.DataFrame):\n",
    "    for i in positively_skewed:\n",
    "        perc = df[df[i] == 0].shape[0] / df.shape[0] * 100\n",
    "        fig, (bef, aft) = plt.subplots(ncols=2)\n",
    "        bef.set_title(label='Before Logarithmic Transformation')\n",
    "        aft.set_title(label='After Logarithmic Transformation')\n",
    "        sns.histplot(data=df, x=i, ax=bef)\n",
    "        sns.histplot(data=np.log1p(df[i]), ax=aft)\n",
    "plot_E(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.corr()['price'].sort_values(key=lambda x: np.abs(x), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.unique(df2['id']).size, df2.count()['id'])\n",
    "df2.drop(labels='id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.unique(df2['zipcode']).size, df2.count()['zipcode'])\n",
    "# sns.relplot(data=df2, x='zipcode', y='price')\n",
    "df2.drop(labels=['id', 'zipcode'], axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in positively_skewed:\n",
    "    fig, (bef, aft, aft_ag) = plt.subplots(ncols=3)\n",
    "    sns.scatterplot(data=df2, x=i, y='price', ax=bef)\n",
    "    sns.scatterplot(x=np.sqrt(df2[i]), y=np.sqrt(df2['price']), ax=aft)\n",
    "    sns.scatterplot(x=(df2[i]), y=np.sqrt(df2['price']), ax=aft_ag)"
   ]
  },
  {
   "source": [
    "#### Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['id', 'zipcode'], axis=1, inplace=True)\n",
    "df_date = pd.to_datetime(df['date'], yearfirst=True)\n",
    "df['year'] = pd.DatetimeIndex(data=df_date).year\n",
    "df['month'] = pd.DatetimeIndex(data=df_date).month\n",
    "df['day'] = pd.DatetimeIndex(data=df_date).day\n",
    "df.drop(labels='date', axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "#### Feature Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_corr = df2.corr()\n",
    "df2_corr.drop(labels=df2_corr.columns[df2_corr.columns != 'price'].values, axis=1).drop(labels='price', axis=0)['price'].sort_values(ascending=False)"
   ]
  },
  {
   "source": [
    "### Data Partitioning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_another = df2.copy()\n",
    "sns.heatmap(data=df2[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'sqft_lot15']].corr(), cmap='RdBu', vmin=-1, vmax=1, annot=True)\n",
    "positively_skewed = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "dfp = df_another['price']\n",
    "df_another = np.log1p(df_another[['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_living15', 'sqft_lot15']])\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(data=df_another.corr(), cmap='RdBu', vmin=-1, vmax=1, ax=ax, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_another.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import QuantileTransformer, FunctionTransformer, RobustScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import validation_curve\n",
    "log_transform = FunctionTransformer(func=np.log10, inverse_func=np.vectorize(lambda x: 10 ** x))\n",
    "drop_inf = FunctionTransformer(func=np.vectorize(lambda x: x if x > 0 else 1))\n",
    "\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "class DummyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self): pass\n",
    "    def fit_transform(self): pass\n",
    "    def transform(self): pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# expt - use df_another\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(df2.drop('price', axis=1), df2['price'], test_size=0.2)\n",
    "\n",
    "u = Pipeline(steps=[\n",
    "    # ('trans', CustomTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reg', DummyEstimator())\n",
    "])\n",
    "\n",
    "\n",
    "params_ = [\n",
    "    {\n",
    "        'reg': [LinearRegression()],\n",
    "        'reg__normalize': [True, False],\n",
    "        'reg__fit_intercept': [True, False]\n",
    "    },\n",
    "    {\n",
    "        'reg': [Lasso(), Ridge()],\n",
    "        'reg__alpha': np.logspace(-5, 3, 6)\n",
    "    },\n",
    "    {\n",
    "        'reg': [DecisionTreeRegressor()],\n",
    "        'reg__max_depth': np.arange(5, 11)\n",
    "    },\n",
    "    {\n",
    "        'reg': [KNeighborsRegressor()],\n",
    "        'reg__n_neighbors': np.arange(5, 11)\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "lol = GridSearchCV(u, params_, n_jobs=3)\n",
    "lol.fit(X3_train, y3_train)\n",
    "\n",
    "results = pd.DataFrame(lol.cv_results_).sort_values(by='rank_test_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=['rank_test_score', 'std_test_score']).iloc[0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(max_depth=9)\n",
    "model.fit(X3_train, y3_train)\n",
    "model.score(X3_train, y3_train), model.score(X3_test, y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pickle.load(file=open('./models/grid_search_reg.p', 'rb'))\n",
    "# pickle.dump(obj=results, file=open('./models/grid_search_reg.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = DecisionTreeRegressor(max_depth=8)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(df2.drop('price', axis=1), df2['price'], test_size=0.25, random_state=777)\n",
    "mod.fit(X3_train, y3_train)\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(mod.predict(X3_test), y3_test), r2_score(mod.predict(X3_train), y3_train)"
   ]
  },
  {
   "source": [
    "After deciding on algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        all_cols = X.columns.values\n",
    "        for i in all_cols:\n",
    "            if i == 'long':\n",
    "                X_copy.drop(labels='long', axis=1, inplace=True)\n",
    "            elif i in positively_skewed:\n",
    "                X_copy[i] = np.log1p(X[i])\n",
    "        return X_copy\n",
    "\n",
    "dtreepipe = Pipeline(steps=[\n",
    "    # ('transformer', CustomTransformer()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "dtreeparams = {\n",
    "    'regressor__max_depth': np.arange(2, 25, 2),\n",
    "    'regressor__min_samples_split': np.arange(10, 31, 10),\n",
    "    'regressor__min_samples_leaf': np.arange(10, 300, 50)\n",
    "}\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "dtreeCV = RandomizedSearchCV(estimator=dtreepipe, param_distributions=dtreeparams, cv=3, n_jobs=-1)\n",
    "\n",
    "dtreeCV.fit(X3_train, y3_train)\n",
    "dtreeCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor\n",
    "\n",
    "class CustomTransformer2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        # X_copy[]\n",
    "        return X_copy\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(df2.drop('price', axis=1), df2['price'], test_size=0.25)\n",
    "\n",
    "latest_dtree = Pipeline(steps=[\n",
    "    # ('cust', CustomTransformer2()),\n",
    "    ('scale', StandardScaler()),\n",
    "    # ('check', CustomTransformer()),\n",
    "    ('reg', GradientBoostingRegressor(max_depth=20, min_samples_leaf=10, min_samples_split=30)\n",
    ")])\n",
    "\n",
    "noscale = Pipeline(steps=[\n",
    "    # ('cust', CustomTransformer()),\n",
    "    # ('scale', StandardScaler()),\n",
    "    ('reg', GradientBoostingRegressor(max_depth=20, min_samples_leaf=10, min_samples_split=30)\n",
    ")])\n",
    "\n",
    "log_price = TransformedTargetRegressor(regressor=latest_dtree, func=np.sqrt, inverse_func=np.square, check_inverse=False)\n",
    "\n",
    "rt_nos = TransformedTargetRegressor(regressor=noscale, func=np.sqrt, inverse_func=np.square, check_inverse=False)\n",
    "\n",
    "mos = ['nope', 'trans', 'noscale', 'noscaler trans']\n",
    "for j, mo in enumerate([latest_dtree, log_price, noscale, rt_nos]):\n",
    "    mo.fit(X3_train, y3_train)\n",
    "    tr_sc = r2_score(y3_train, mo.predict(X3_train))\n",
    "    ts_sc = r2_score(y3_test, mo.predict(X3_test))\n",
    "    print(mos[j], tr_sc - ts_sc, tr_sc, ts_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEST CASE\n",
    "# ogp = Pipeline(steps=[\n",
    "#     ('scale', StandardScaler()),\n",
    "#     ('reg', BaggingRegressor(GradientBoostingRegressor(max_depth=20, min_samples_leaf=10, min_samples_split=30), n_jobs=-1)\n",
    "# )])\n",
    "\n",
    "# bestboi = TransformedTargetRegressor(regressor=bag, func=np.sqrt, inverse_func=np.square, check_inverse=False)\n",
    "\n",
    "# bestboi.fit(X3_train, y3_train)\n",
    "\n",
    "# bestboi.score(X3_test, y3_test)\n",
    "\n",
    "# pickle.dump(obj=bestboi, file=open('./models/best_reg_model.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = pickle.load(file=open('./models/best_reg_model.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sample(frac=1)"
   ]
  },
  {
   "source": [
    "### Model Scoring"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conclusions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}